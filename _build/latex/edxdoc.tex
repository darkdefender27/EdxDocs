% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[a4paper,12pt,oneside]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}


\title{edX Analytics Documentation}
\date{July 02, 2014}
\release{1.0.1}
\author{Edx}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


Contents:


\chapter{\textbf{Abstract}}
\label{document:welcome-to-edx-analytics-documentation}\label{document:abstract}\label{document::doc}
IIT Bombay now has its contributions(in the form of two courses) to the prestigious edX platform. EdX is a massive open online course (MOOC) destination site and online learning platform that is open sourced, founded by the Massachusetts Institute of Technology and Harvard University. Though it is an ITS(Intelligent Tutoring System), students find out some or the other way to play around with the features of the system through which they reach the final(correct) answer without even thinking deeply about the question asked. So to avoid such instances, we need to determine those students who are gaming the system(behavior aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software’s feedback and help). For this, we analyzed the log files generated by the student activities in the ITS (Intelligent tutoring system) i.e, the edX course site and extracted some features relevant to finding out the students who are gaming the system. Determination of such students will help us in the future to intervene the learning process, so as to prevent such students from  gaming and help them in learning the course.

We have also written queries to extract information which will be useful for data analytics. Using this data, the ITS designer and the course instructors can get useful information as to how well the students are giving positive response to the system and how popular and effecive their courses are. To aid them, a visualized interface has been designed so that it becomes easier and graphically legible to infer such aspects of a student's interest.


\chapter{\textbf{INTRODUCTION}}
\label{document:introduction}

\section{What is data analytics and its importance?}
\label{document:what-is-data-analytics-and-its-importance}
Data analyticis is a practice in which raw data is ordered and organized so that useful information can be extracted from it. The process of organizing and thinking about data is key to understanding what the data does and does not contain. There are a variety of ways in which people can approach data analysis, and it is notoriously easy to manipulate data during the analysis phase to push certain conclusions or agendas. For this reason, it is important to pay attention when data analysis is presented, and to think critically about the data and the conclusions which were drawn.
Here we are going to analyse the log data which are generated when a student takes any course on edx . Analysing this data , we are going to reach to the conclusion whether a student is gaming the system , while taking the course.If we are successful in determing , which students are gaming , we can apply proper interventions against them.This project can be of great use to ITS in implemetning trustworthy courses , so that the certificates re assigned to only deserving students who have done their course seriously and honestly.We are first going to analyse these data and then apply machine learning in these in order to determine properly which students have been gaming the system.


\section{Why data analysis in educational data?}
\label{document:why-data-analysis-in-educational-data}
In education, the use of data and analytics to improve learning is referred to as learning analytics. Analytics have not yet made the impact on education that they have made in other fields. That’s starting to change. Software companies, researchers, educators, and university leaders recognize the value of data in improving not only teaching and learning, but the entire education sector. In particular, learning analytics enables universities, schools, and corporate training departments to improve the quality of learning and overall competitiveness.

The log files have entries for every activity ever made by a student on the edX platform. Each and every click on tabs or question attempted or a video being played, all the entries are created in the log file. From this log file, we parse out the relevant information and find out patterns to detect that a student is showing any actions involving gaming the system. Gaming, as described above, is any behaviour aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software’s feedback and help. Thus, classifying the gaming students is our priority. This involves careful feature extraction, data analytics and machine learning on the features for classification.
Feedback to the ITS designer and course tutor is equally important. It is essential to know whether the system is doing a good work or not. This involves calculating a student's response based on various categories. A visualized form of such information is useful to them for deciding what reformations they need to apply, so as to make the course even more compelling and interesting to the students.


\section{\textbf{PURPOSE}}
\label{document:purpose}
The main purpose of our project is to determine which student is gaming the system so that we can apply interventions in their tutor system. For example if a question has some hint options, then a student can repeatedly use that hint option to reach to the correct answer. But, this kind of behaviour tampers the process of learning of the student. So, our main aim is to filter out such students who get involved in off-task behaviours and implement interventions only on those students' tutor system, which would passify their gaming process and indirectly motivate them to learn and attempt the quizes whole-heartedly. To implement such a system, relevant feature extraction from the log files is a necessity. Then comes the use of machine learning to classify a student as gaming or not gaming based on the features extracted.

Secondly, data analytics also involves a thorough study of the database containing all the demographic and activity information of a user. From this data, one can infer as to which category of students are mostly interested in learning the course. This requires queries to be written on that data from which we can extract relevant information. A visualized form of such information needs to be created as a feedback to the course designers. Looking at this, they can decide what reformations they need to apply, so as to make the course seem even more compelling and interesting to the students.


\section{\textbf{SCOPE}}
\label{document:scope}
Implementing the ideas described above will make the tutoring system very efficient while grading a student. Any normal ITS, without any provisions for detection of a student who is gaming and implementing interventions in their system, will award a certificate to any student who has completed the course. But this would make a system very incompetent in correctly grading an undeserving student. Thus our idealogies would aid the ITS in fairly classifying the students into gaming and not gaming and accrordingly award the certificates to the deserving students. In addition, the tutoring system will be able to pick out the `gaming' students and interevene their learning process so that their learning skills also match the regular students' learning, thereby widening the scope of the efficiency of the edX course site.


\chapter{\textbf{Overall description}}
\label{document:overall-description}
EdX-data analyzer uses data genreated by edX  in the form of log entries and the database it creates. Data containing information related to the students is stored in the database `edxapp'. Data from  edxapp is used for analysis purposes like number of dropouts according to education level, location, gender, number of students according to their eduation level enrolled in a particular course.
Log entries genrated by the server will help us to find if the student is gaming the system or not. EdX-data analyzer will constantly look for the new entries  in the log files and parse, process  them to find wheather students are gaming or not. EdX-data analyzer parses the log file entries and stores them using hive on hadoop distributed file system.


\section{\textbf{Product Functions}}
\label{document:product-functions}
EdX-data analyzer serves main purpose of determining wheather the student is gaming the system or not. It can be used to interrupt the student who are tring to game the system and adjust the tutor system such that it will be difficult for the student to game the system.


\section{\textbf{Constraints}}
\label{document:constraints}
Analysing the log data to find whether the student is gaming the system or not involves many constriants like diffculties in predicting the state of mind of person by just looking at it's interaction with the system. It is not possible to determine if a student is sleeping while watching the video or whether a student is paying proper attention. Along with difficulties in predicting the state of mind of the student, it is also not possible to note down each and every action or interaction of the user with his/her system due to privacy policies. Instance of this diffcutly can be a situation for example, a student who pauses the video might be pausing the video and indulging in the other off task behavior or a student might be getting confused while watching the video and now try to understand the concept over the internet. As seen in the example, it is difficult to predict the extact state mind of the student. To determine whether the student is gaming or not, detailed analysis is required. Even if we are able to determine whether the student is gaming or not,  we need to take some action to prevent the student from the gaming. But, question is how to determine which steps should be taken to stop the student from gaming as it will depend on the reason behind the gaming which is a furture part of analysis not covered here.


\section{\textbf{Assumptions and Dependancies}}
\label{document:assumptions-and-dependancies}
EdX-data analyzer assumes that log entries genrated by the EdX ITS server are error free. As only source of input to EdX-data analyzer is data provided by EdX ITS server. EdX-data analyzer totally depends on the EdX ITS for the data.


\chapter{\textbf{Technologies used:}}
\label{document:technologies-used}
Apache Hadoop is an open source software project that enables the distributed processing of large data sets across clusters of commodity servers. It is designed to scale up from a single server to thousands of machines, with a very high degree of fault tolerance. Rather than relying on high-end hardware, the resiliency of these clusters comes from the software’s ability to detect and handle failures at the application layer.

Apache Hadoop has two main subprojects:

MapReduce - The framework that understands and assigns work to the nodes in a cluster.
HDFS - A file system that spans all the nodes in a Hadoop cluster for data storage. It links together the file systems on many local nodes to make them into one big file system. HDFS assumes nodes will fail, so it achieves reliability by replicating data across multiple nodes

Hive is a runtime Hadoop support structure that allows anyone who is already fluent with SQL (which is commonplace for relational data-base developers) to leverage the Hadoop platform right out of the gate.
Hive allows SQL developers to write Hive Query Language (HQL) statements that are similar to standard SQL statements. HQL is limited in the commands it understands, but it is still useful. HQL statements are broken down by the Hive service into MapReduce jobs and executed across a Hadoop cluster.

Sqoop is a command-line interface application for transferring data between relational databases and Hadoop. It supports incremental loads of a single table or a free form SQL query as well as saved jobs which can be run multiple times to import updates made to a database since the last import. Imports can also be used to populate tables in Hive or HBase. Exports can be used to put data from Hadoop into a relational database.


\chapter{REQUIREMENTS}
\label{document:requirements}

\section{\textbf{Functional Requirements}}
\label{document:functional-requirements}\begin{enumerate}
\item {} 
The system shall analyse the data based on various parameters of the student such as location,age group,gender etc.

\item {} 
The instructor shall be able to choose the comparison parameters and input valid entries to be queried. The instructor shall be able to input the subject for the data to be queried.

\item {} 
The data shall be represented in visual format to be understood by the instructor. The visuals formats may include pie-charts,bar charts, line charts etc.

\item {} 
The system shall parse the log data and store the parsed data into relevant event related tables.

\item {} 
The system shall extract the relevant and useful data from the parsed data.

\item {} 
The system shall tell whether a student is gaming a system or not. The system shall do this after analysing the various actions performed by the student while giving the test.

\end{enumerate}


\section{\textbf{Performance Requirements}}
\label{document:performance-requirements}\begin{enumerate}
\item {} 
The edx analytics shall support in courses having large number of students (in thousands). There shall be minimal delay in retrieving the data.

\item {} 
The analysis shall be done on the data which has not been processed,i.e,only new data shall be considered for analysis. This would avoid the reading of unneccesary data again and again. This would be called as incremental implementation of queries.

\end{enumerate}


\section{\textbf{Non Functional Requirments}}
\label{document:non-functional-requirments}\begin{enumerate}
\item {} 
The visual diagrams displaying the analysis with various parameters of the student shall be in a easy form so as to be understood by each and every instructor including those belonging to non-mathematical back-ground.

\end{enumerate}


\chapter{HADOOP}
\label{document:hadoop}
This section refers to the installation settings of Hadoop on a standalone system
as well as on a system existing as a node in a cluster.


\section{Running Hadoop on Ubuntu (Single node cluster setup)}
\label{document:running-hadoop-on-ubuntu-single-node-cluster-setup}
The report here will describe the required steps for setting up a single-node Hadoop cluster backed by the Hadoop Distributed File System, running on Ubuntu Linux.
Hadoop is a framework written in Java for running applications on large clusters of commodity hardware and incorporates features similar to those of the Google File System (GFS) and of the MapReduce computing paradigm. Hadoop’s HDFS is a highly fault-tolerant distributed file system and, like Hadoop in general, designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications that have large data sets.

Before we start, we will understand the meaning of the following:


\subsection{DataNode:}
\label{document:datanode}
A DataNode stores data in the Hadoop File System. A functional file system has more than one DataNode, with the data replicated across them.


\subsection{NameNode:}
\label{document:namenode}
The NameNode is the centrepiece of an HDFS file system. It keeps the directory of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these file itself.


\subsection{Jobtracker:}
\label{document:jobtracker}
The Jobtracker is the service within hadoop that farms out MapReduce to specific nodes in the cluster, ideally the nodes that have the data, or atleast are in the same rack.


\subsection{TaskTracker:}
\label{document:tasktracker}
A TaskTracker is a node in the cluster that accepts tasks- Map, Reduce and Shuffle operatons – from a Job Tracker.


\subsection{Secondary Namenode:}
\label{document:secondary-namenode}
Secondary Namenode whole purpose is to have a checkpoint in HDFS. It is just a helper node for namenode.


\section{PREREQUISITES}
\label{document:prerequisites}

\subsection{Java 6 JDK}
\label{document:java-6-jdk}
Hadoop requires a working Java 1.5+ (aka Java 5) installation.

Update the source list

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo apt\PYGZhy{}get update
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{1.png}}
\end{figure}

or

Install Sun Java 6 JDK
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{2.png}}
\end{figure}


\subsubsection{Note:}
\label{document:note}
If you already have Java JDK installed on your system, then you need not run the above command.

To install it

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo apt\PYGZhy{}get install sun\PYGZhy{}java6\PYGZhy{}jdk
\end{Verbatim}

The full JDK which will be placed in /usr/lib/jvm/java-6-openjdk-amd64
After installation, check whether java JDK is correctly installed or not, with the following command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} java \PYGZhy{}version
\end{Verbatim}


\subsection{Adding a dedicated Hadoop system user}
\label{document:adding-a-dedicated-hadoop-system-user}
We will use a dedicated Hadoop user account for running Hadoop.

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo addgroup hadoop\PYGZus{}group
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{3.png}}
\end{figure}

Command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo addgroup hadoop\PYGZus{}group
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{4.png}}
\end{figure}

This will add the user hduser1 and the group hadoop\_group to the local machine.
Add hduser1 to the sudo group

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo adduser hduser1 sudo
\end{Verbatim}


\subsection{Configuring SSH}
\label{document:configuring-ssh}
The hadoop control scripts rely on SSH to peform cluster-wide operations. For example, there is a script for stopping and starting all the daemons in the clusters. To work seamlessly, SSh needs to be etup to allow password-less login for the hadoop user from machines in the cluster. The simplest ay to achive this is to generate a public/private key pair, and it will be shared across the cluster.

Hadoop requires SSH access to manage its nodes, i.e. remote machines plus your local machine. For our single-node setup of Hadoop, we therefore need to configure SSH access to localhost for the hduser user we created in the earlier.

We have to generate an SSH key for the hduser1 user.

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} su – hduser1
hduser1@ubuntu:\PYGZti{}\PYGZdl{} ssh\PYGZhy{}keygen \PYGZhy{}t rsa \PYGZhy{}P \PYGZdq{}\PYGZdq{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{5.png}}
\end{figure}

The second line will create an RSA key pair with an empty password.


\subsubsection{Note:}
\label{document:id1}
P “”, here indicates an empty password

You have to enable SSH access to your local machine with this newly created key which is done by the following command.

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{}   cat \PYGZdl{}HOME/.ssh/id\PYGZus{}rsa.pub \PYGZgt{}\PYGZgt{} \PYGZdl{}HOME/.ssh/authorized\PYGZus{}keys
\end{Verbatim}

The final step is to test the SSH setup by connecting to the local machine with the hduser1 user.
The step is also needed to save your local machine’s host key fingerprint to the hduser1 user’s known hosts file.

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} ssh localhost
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{6.png}}
\end{figure}

If the SSH connection fails, we can try the following (optional):
\begin{itemize}
\item {} 
Enable debugging with ssh -vvv localhost and investigate the error in detail.

\item {} 
Check the SSH server configuration in /etc/ssh/sshd\_config.  If you made any changes to the SSH server configuration file, you can force a configuration reload with sudo /etc/init.d/ssh reload.

\end{itemize}


\section{INSTALLATION}
\label{document:installation}

\subsection{Main Installation}
\label{document:main-installation}\begin{itemize}
\item {} 
Now, I will start by switching to hduser1

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} su \PYGZhy{} hduser1
\end{Verbatim}

\item {} 
Now, download and extract Hadoop 1.2.0

\end{itemize}

We now have to move the extracted folder to the directory /usr/local
The command to be used:

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo mv /home/a\PYGZus{}user/Downloads/hadoop /usr/local
\end{Verbatim}

Note: a\_user is a user registered to ubuntu.
\begin{itemize}
\item {} 
Setup Environment Variables for Hadoop

\end{itemize}

Add the following entries to .bashrc file, here we use the nano editor

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{}  sudo nano \PYGZti{}/.bashrc
\end{Verbatim}

Add the following entries to .bashrc file

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Set Hadoop\PYGZhy{}related environment variables
export HADOOP\PYGZus{}HOME=/usr/local/hadoop
\PYGZsh{} Add Hadoop bin/ directory to PATH
export PATH= \PYGZdl{}PATH:\PYGZdl{}HADOOP\PYGZus{}HOME/bin
\end{Verbatim}


\subsection{Configuration}
\label{document:configuration}

\subsubsection{hadoop-env.sh}
\label{document:hadoop-env-sh}
Replace current JAVA\_HOME value in
/usr/local/hadoop/etc/hadoop/hadoop-env.sh with /usr/lib/jvm/jdk/

Steps:
\begin{enumerate}
\item {} 
Command

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo nano /usr/local/hadoop/conf/hadoop\PYGZhy{}env.sh
\end{Verbatim}

\item {} 
Edit the value of JAVA\_HOME with /usr/lib/jvm/jdk(version you have installed)

\item {} 
The jdk above refers to the java JDK you hav installed.

\end{enumerate}

OR (depending on the version of Hadoop you download)

Change the file:
conf/hadoop-env.sh

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c}{\PYGZsh{}export JAVA\PYGZus{}HOME=/usr/lib/j2sdk1.5\PYGZhy{}sun}
\end{Verbatim}

to
in the same file

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c}{\PYGZsh{} export JAVA\PYGZus{}HOME=/usr/lib/jvm/java\PYGZhy{}6\PYGZhy{}openjdk\PYGZhy{}amd64  (for 64 bit)}
\PYG{c}{\PYGZsh{} export JAVA\PYGZus{}HOME=/usr/lib/jvm/java\PYGZhy{}6\PYGZhy{}openjdk\PYGZhy{}amd64  (for 32 bit)}
\end{Verbatim}


\subsubsection{conf/{\color{red}\bfseries{}*}-site.xml}
\label{document:conf-site-xml}
Now we create the directory and set the required ownerships and permissions

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo mkdir \PYGZhy{}p /app/hadoop/tmp
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo chown hduser:hadoop /app/hadoop/tmp
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo chmod 750 /app/hadoop/tmp
\end{Verbatim}

The last line gives reading and writing permissions to the /app/hadoop/tmp directory
\begin{itemize}
\item {} 
\textbf{Error}: If you forget to set the required ownerships and permissions, you will see a java.io.IO Exception when you try to format the name node.

\end{itemize}

Open the fllowing file using

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo nano *\PYGZhy{}site.xml
\end{Verbatim}

Paste the following between \textless{}configuration\textgreater{}
\begin{itemize}
\item {} 
\textbf{In file conf/core-site.xml}

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}hadoop.tmp.dir\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}/app/hadoop/tmp\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}A base for other temporary directories.\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}fs.default.name\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}hdfs://localhost:54310\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The name of the default file system.  A URI whose
    scheme and authority determine the FileSystem implementation.  The
    uri\PYGZsq{}s scheme determines the config property (fs.SCHEME.impl) naming
    the FileSystem implementation class.  The uri\PYGZsq{}s authority is used to
    determine the host, port, etc. for a filesystem.\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}

\item {} 
\textbf{In file conf/mapred-site.xml}

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
\PYGZlt{}name\PYGZgt{}mapred.job.tracker\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}localhost:54311\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The host and port that the MapReduce job tracker runs
    at.  If \PYGZdq{}local\PYGZdq{}, then jobs are run in\PYGZhy{}process as a single map
    and reduce task.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}

\item {} 
\textbf{In file conf/hdfs-site.xml}

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}dfs.replication\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}1\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}

\end{itemize}


\subsection{Formatting the HDFS filesystem via the NameNode}
\label{document:formatting-the-hdfs-filesystem-via-the-namenode}
To format the filesystem (which simply initializes the directory specified by the dfs.name.dir variable).
Run the command

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} /usr/local/hadoop/bin/hadoop namenode –format
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{7.png}}
\end{figure}


\subsection{Starting your single-node cluster}
\label{document:starting-your-single-node-cluster}
Before starting the cluster, we need to give the required permissions to the directory with the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo chmod \PYGZhy{}R 777 /usr/local/hadoop
\end{Verbatim}

Run the command

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} /usr/local/hadoop/bin/start\PYGZhy{}all.sh
\end{Verbatim}

This will startup a Namenode, Datanode, Jobtracker and a Tasktracker on the machine.

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:/usr/local/hadoop\PYGZdl{} jps
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{8.png}}
\end{figure}


\subsubsection{Errors:}
\label{document:errors}\begin{enumerate}
\item {} \begin{description}
\item[{If by chance your datanode is not starting, then you have to erase the contents of the folder /app/hadoop/tmp}] \leavevmode
The command that can be used

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}:\PYGZdl{} sudo rm –Rf /app/hadoop/tmp/*
\end{Verbatim}

\end{description}

\item {} \begin{description}
\item[{You can also check with netstat if Hadoop is listening on the configured ports.}] \leavevmode
The command that can be used

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} sudo netstat \PYGZhy{}plten \textbar{} grep java
\end{Verbatim}

\end{description}

\item {} 
Errors if any, examine the log files in the /logs/ directory.

\end{enumerate}


\subsection{Stopping your single-node cluster}
\label{document:stopping-your-single-node-cluster}
Run the command to stop all the daemons running on your machine.

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} /usr/local/hadoop/bin/stop\PYGZhy{}all.sh
\end{Verbatim}


\subsubsection{ERROR POINTS:}
\label{document:error-points}
If datanode is not starting, then clear the tmp folder before formatting the namenode using the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{} rm \PYGZhy{}Rf /app/hadoop/tmp/*
\end{Verbatim}


\subsubsection{Note:}
\label{document:id4}\begin{itemize}
\item {} 
The masters and slaves file should contain localhost.

\item {} 
In /etc/hosts, the ip of the system should be given with the alias as localhost.

\item {} 
Set the java home path in hadoop-env.sh as well bashrc.

\end{itemize}


\paragraph{MULTI-NODE INSTALLATION}
\label{document:multi-node-installation}

\section{Running Hadoop on Ubuntu Linux (Multi-Node Cluster)}
\label{document:running-hadoop-on-ubuntu-linux-multi-node-cluster}

\subsection{From single-node clusters to a multi-node cluster}
\label{document:from-single-node-clusters-to-a-multi-node-cluster}
We will build a multi-node cluster merge two or more single-node clusters into one multi-node cluster in which one Ubuntu box will become the designated master but also act as a slave , and the other box will become only a slave.
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{9.png}}
\end{figure}


\section{Prerequisites}
\label{document:id5}
Configuring single-node clusters first, here we have used two single node clusters.
Shutdown each single-node cluster with the following command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  bin/stop\PYGZhy{}all.sh
\end{Verbatim}


\section{Networking}
\label{document:networking}\begin{itemize}
\item {} 
The easiest is to put both machines in the same network with regard to hardware and   software configuration.

\item {} 
Update /etc/hosts on both machines .Put the alias to the ip addresses of all the machines. Here we are creating a cluster of 2 machines , one is master and other is slave 1

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZdl{}  cd /etc/hosts
\end{Verbatim}

\item {} 
Add the following lines for two node cluster

\begin{Verbatim}[commandchars=\\\{\}]
10.105.15.78    master  (IP address of the master node)
10.105.15.43    slave1   (IP address of the slave node)
\end{Verbatim}

\end{itemize}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{10.png}}
\end{figure}


\section{SSH access}
\label{document:ssh-access}
The hduser user on the master (aka \href{mailto:hduser@master}{hduser@master}) must be able to connect:
\begin{enumerate}
\item {} 
to its own user account on the master - i.e. ssh master in this context.

\item {} 
to the hduser user account on the slave (i.e. \href{mailto:hduser@slave1}{hduser@slave1}) via a password-less SSH login.

\end{enumerate}
\begin{itemize}
\item {} 
Add the \href{mailto:hduser@master}{hduser@master} public SSH key using the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  ssh\PYGZhy{}copy\PYGZhy{}id \PYGZhy{}i \PYGZdl{}HOME/.ssh/id\PYGZus{}rsa.pub hduser@slave1
\end{Verbatim}

\end{itemize}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{11.png}}
\end{figure}
\begin{itemize}
\item {} 
Connect with user hduser from the master to the user account hduser on the slave.

\end{itemize}
\begin{enumerate}
\item {} 
From master to master

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  ssh master
\end{Verbatim}

\end{enumerate}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{12.png}}
\end{figure}
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
From master to slave

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  ssh slave1
\end{Verbatim}

\end{enumerate}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{13.png}}
\end{figure}


\section{Hadoop}
\label{document:id6}

\subsection{Cluster Overview}
\label{document:cluster-overview}
This will describe how to configure one Ubuntu box as a master node and the other Ubuntu box as a slave node.


\subsection{Configuration}
\label{document:id7}

\subsubsection{conf/masters}
\label{document:conf-masters}
The machine on which bin/start-dfs.sh is running will become the primary NameNode.
This file should be updated on all the nodes. Open the masters file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master/slave :\PYGZti{}\PYGZdl{} /usr/local/hadoop/conf
hduser@master/slave :\PYGZti{}\PYGZdl{} sudo gedit masters
\end{Verbatim}

Add the following line

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{Master}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{14.png}}
\end{figure}


\subsubsection{conf/slaves}
\label{document:conf-slaves}
This file should be updated on all the nodes as master is also a slave.
Open the slaves file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master/slave:\PYGZti{}/usr/local/hadoop/conf\PYGZdl{} sudo gedit slaves
\end{Verbatim}

Add the following lines

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{Master}
\PYG{n}{Slave1}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{15.png}}
\end{figure}


\subsubsection{conf/{\color{red}\bfseries{}*}-site.xml (all machines)}
\label{document:conf-site-xml-all-machines}

\subsubsection{conf/core-site.xml}
\label{document:conf-core-site-xml}
Open this file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}/usr/local/hadoop/conf\PYGZdl{} sudo gedit core\PYGZhy{}site.xml
\end{Verbatim}

Change the fs.default.name parameter (in conf/core-site.xml), which specifies the NameNode (the HDFS master) host and port.

conf/core-site.xml (ALL machines .ie. Master as well as slave)

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}fs.default.name\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}hdfs://master:54310\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The name of the default file system.  A URI whose
    scheme and authority determine the FileSystem implementation.  The
    uri\PYGZsq{}s scheme determines the config property (fs.SCHEME.impl) naming
    the FileSystem implementation class.  The uri\PYGZsq{}s authority is used to
    determine the host, port, etc. for a filesystem.\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{16.png}}
\end{figure}


\subsubsection{conf/mapred-site.xml}
\label{document:conf-mapred-site-xml}
Open this file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  /usr/local/hadoop/conf
hduser@master:\PYGZti{}\PYGZdl{}  sudo gedit mapred\PYGZhy{}site.xml
\end{Verbatim}

Change the mapred.job.tracker parameter (in conf/mapred-site.xml), which specifies the JobTracker (MapReduce master) host and port.

conf/mapred-site.xml (ALL machines)

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}mapred.job.tracker\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}master:54311\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The host and port that the MapReduce job tracker runs
    at.  If \PYGZdq{}local\PYGZdq{}, then jobs are run in\PYGZhy{}process as a single map
    and reduce task.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{17.png}}
\end{figure}


\subsubsection{conf/hdfs-site.xml}
\label{document:conf-hdfs-site-xml}
Open this file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  /usr/local/hadoop/conf
hduser@master:\PYGZti{}\PYGZdl{}  sudo gedit hdfs\PYGZhy{}site.xml
\end{Verbatim}

Change the dfs.replication parameter (in conf/hdfs-site.xml) which specifies the default block replication.
We have two nodes available, so we set dfs.replication to 2.


\subsubsection{conf/hdfs-site.xml (ALL machines)}
\label{document:conf-hdfs-site-xml-all-machines}
Changes to be made

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
\PYGZlt{}name\PYGZgt{}dfs.replication\PYGZlt{}/name\PYGZgt{}
\PYGZlt{}value\PYGZgt{}2\PYGZlt{}/value\PYGZgt{}
\PYGZlt{}description\PYGZgt{}Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{18.png}}
\end{figure}


\subsection{Formatting the HDFS filesystem via the NameNode}
\label{document:id10}
Format the cluster’s HDFS file system

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}/usr/local/hadoop\PYGZdl{} bin/hadoop namenode \PYGZhy{}format
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{19.png}}
\end{figure}


\subsection{Starting the multi-node cluster}
\label{document:starting-the-multi-node-cluster}
Starting the cluster is performed in two steps.
\begin{enumerate}
\item {} 
We begin with starting the HDFS daemons: the NameNode daemon is started on master, and DataNode daemons are started on all slaves (here: master and slave).

\item {} 
Then we start the MapReduce daemons: the JobTracker is started on master, and TaskTracker daemons are started on all slaves (here: master and slave).

\end{enumerate}

Cluster is started by running the commnd on master

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}   /usr/local/hadoop
hduser@master:\PYGZti{}\PYGZdl{}   bin/start\PYGZhy{}all.sh
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{20.png}}
\end{figure}

By this command:
\begin{itemize}
\item {} 
The NameNode daemon is started on master, and DataNode daemons are started on all slaves (here: master and slave).

\item {} 
The JobTracker is started on master, and TaskTracker daemons are started on all slaves (here: master and slave)

\end{itemize}

To check the daemons running , run the following commands

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  jps
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{21.png}}
\end{figure}

On slave, datanode and jobtracker should run.

\begin{Verbatim}[commandchars=\\\{\}]
hduser@slave:\PYGZti{}/usr/local/hadoop\PYGZdl{} jps
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{22.png}}
\end{figure}


\subsection{Stopping the multi-node cluster}
\label{document:stopping-the-multi-node-cluster}
To stop the multinode cluster , run the following command on master pc

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{} cd /usr/local/hadoop
hduser@master:\PYGZti{}/usr/local/hadoop\PYGZdl{} bin/stop\PYGZhy{}all.sh
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{23.png}}
\end{figure}


\subsubsection{ERROR POINTS:}
\label{document:id11}\begin{enumerate}
\item {} \begin{description}
\item[{Number of slaves = Number of replications in hdfs-site.xml}] \leavevmode
also number of slaves = all slaves + master(if master is also considered to be a slave)

\end{description}

\item {} 
When you start the cluster, clear the tmp directory on all the nodes (master+slaves) using the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}   rm \PYGZhy{}Rf /app/hadoop/tmp/*
\end{Verbatim}

\item {} 
Configuration of /etc/hosts , masters  and slaves files on both the masters and the slaves nodes should be the same.

\item {} 
If namenode is not getting started run the following commands:
\begin{itemize}
\item {} 
To give all permissions  of hadoop folder to hduser

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  sudo chmod \PYGZhy{}R 777 /app/hadoop
\end{Verbatim}

\item {} 
This command deletes the junk files which gets stored in tmp folder of hadoop

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  sudo rm \PYGZhy{}Rf /app/hadoop/tmp/*
\end{Verbatim}

\end{itemize}

\end{enumerate}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{24.png}}
\end{figure}


\chapter{HIVE}
\label{document:hive}
This section refers to the installation settings of Hive on a standalone system
as well as on a system existing as a node in a cluster.
\begin{quote}

Apache Hive is a data warehouse infrastructure built on top of Hadoop for providing data summarization, query, and analysis. Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 filesystem. It provides an SQL-like language called HiveQL(Hive Query Language) while maintaining full support for map/reduce.
\end{quote}


\section{Installing HIVE:}
\label{document:installing-hive}\begin{itemize}
\item {} 
Browse to the link: \href{http://apache.claz.org/hive/stable/}{http://apache.claz.org/hive/stable/}

\item {} 
Click the apache-hive-0.13.0-bin.tar.gz

\item {} 
Save and Extract it
\begin{quote}

Commands

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  cd  /usr/lib/
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo mkdir hive
user@ubuntu:\PYGZti{}\PYGZdl{}  cd Downloads
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo mv apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin /usr/lib/hive
\end{Verbatim}
\end{quote}

\end{itemize}


\section{Setting Hive environment variable:}
\label{document:setting-hive-environment-variable}
Commands

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  cd
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo gedit  \PYGZti{}/.bashrc
\end{Verbatim}

Copy and paste the following lines at end of the file

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Set HIVE\PYGZus{}HOME
export HIVE\PYGZus{}HOME=\PYGZdq{}/usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin\PYGZdq{}
PATH=\PYGZdl{}PATH:\PYGZdl{}HIVE\PYGZus{}HOME/bin
export PATH
\end{Verbatim}


\section{Setting HADOOP\_PATH in HIVE config.sh}
\label{document:setting-hadoop-path-in-hive-config-sh}
Commands

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} cd  /usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin/bin
user@ubuntu:\PYGZti{}\PYGZdl{} sudo gedit hive\PYGZhy{}config.sh
\end{Verbatim}

Go to the line where the following statements are written

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Allow alternate conf dir location.
HIVE\PYGZus{}CONF\PYGZus{}DIR=\PYGZdq{}\PYGZdl{}\PYGZob{}HIVE\PYGZus{}CONF\PYGZus{}DIR:\PYGZhy{}\PYGZdl{}HIVE\PYGZus{}HOME/conf\PYGZdq{}
export HIVE\PYGZus{}CONF\PYGZus{}DIR=\PYGZdl{}HIVE\PYGZus{}CONF\PYGZus{}DIR
export HIVE\PYGZus{}AUX\PYGZus{}JARS\PYGZus{}PATH=\PYGZdl{}HIVE\PYGZus{}AUX\PYGZus{}JARS\PYGZus{}PATH
\end{Verbatim}

Below this write the following

\begin{Verbatim}[commandchars=\\\{\}]
export HADOOP\PYGZus{}HOME=/usr/local/hadoop
(write the path where hadoop file is there)
\end{Verbatim}


\section{Create Hive directories within HDFS}
\label{document:create-hive-directories-within-hdfs}
Command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}   hadoop fs \PYGZhy{}mkdir /usr/hive/warehouse
\end{Verbatim}


\section{Setting READ/WRITE permission for table}
\label{document:setting-read-write-permission-for-table}
Command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  hadoop fs \PYGZhy{}chmod g+w /usr/hive/warehouse
\end{Verbatim}


\section{HIVE launch}
\label{document:hive-launch}
Command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  hive
\end{Verbatim}

Hive shell will prompt:


\subsection{OUTPUT}
\label{document:output}
Shell will look like

\begin{Verbatim}[commandchars=\\\{\}]
Logging initialized using configuration in
jar:file:/usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin
/lib/hive\PYGZhy{} common\PYGZhy{}0.13.0.jar!/hive\PYGZhy{}log4j.properties
hive\PYGZgt{}
\end{Verbatim}


\section{Creating a database}
\label{document:creating-a-database}
Command

\begin{Verbatim}[commandchars=\\\{\}]
hive\PYGZgt{} create database mydb;
\end{Verbatim}

OUTPUT

\begin{Verbatim}[commandchars=\\\{\}]
OK
Time taken: 0.369 seconds
hive\PYGZgt{}
\end{Verbatim}


\section{Configuring hive-site.xml:}
\label{document:configuring-hive-site-xml}
Open with text-editor and change the following property

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}hive.metastore.local\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}TRUE\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}controls whether to connect to remove metastore server
    or open a new metastore server in Hive Client JVM\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}javax.jdo.option.ConnectionURL\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}jdbc:mysql://usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin/metastore\PYGZus{}db?
    createDatabaseIfNotExist=true\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}JDBC connect string for a JDBC metastore\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}javax.jdo.option.ConnectionDriverName\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}com.mysql.jdbc.Driver\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}Driver class name for a JDBC metastore\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}hive.metastore.warehouse.dir\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}/usr/hive/warehouse\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}location of default database for the warehouse\PYGZlt{}/description\PYGZgt{}
 \PYGZlt{}/property\PYGZgt{}
\end{Verbatim}


\section{Writing a Script}
\label{document:writing-a-script}
Open a new terminal (CTRL+ALT+T)

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}      sudo gedit sample.sql

create database sample;
use sample;
create table product(product int, productname string, price float)
[row format delimited fields terminated by \PYGZsq{},\PYGZsq{};]
describe product;
\end{Verbatim}

load data local inpath `/home/hduser/input\_to\_product.txt' into table product

\begin{Verbatim}[commandchars=\\\{\}]
select * from product;
\end{Verbatim}

SAVE and CLOSE

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo gedit input\PYGZus{}to\PYGZus{}product.txt
user@ubuntu:\PYGZti{}\PYGZdl{} cd /usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin/ \PYGZdl{} bin/hive \PYGZhy{}f
/home/hduser/sample.sql
\end{Verbatim}


\chapter{SQOOP}
\label{document:sqoop}
This section refers to the installation settings of Sqoop.
\begin{figure}[htbp]
\centering

\includegraphics{pik2.png}
\end{figure}


\section{INTRODUCTION}
\label{document:id12}\begin{itemize}
\item {} 
Sqoop is a tool designed to transfer data between Hadoop and relational databases.

\item {} 
You can use Sqoop to import data from a relational database management system(RDBMS) such as MySQL or Oracle into the Hadoop Distributed File System (HDFS), transform the data in Hadoop MapReduce, and then export the data back into an RDBMS. Sqoop automates most of this process, relying on the database to describe the schema for the data to be imported. Sqoop uses MapReduce to import and export the data, which provides parallel operation as well as fault tolerance. This document describes how to get started using Sqoop to move data between databases and Hadoop and provides reference information for the operation of the Sqoop command-line tool suite.

\end{itemize}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{25.png}}
\end{figure}


\section{Stable release and Download}
\label{document:stable-release-and-download}
Sqoop is an open source software product of the Apache Software Foundation.
Sqoop source code is held in the Apache Git repository.


\section{Prerequisites}
\label{document:id13}
Before we can use Sqoop, a release of Hadoop must be installed and configured. Sqoop is currently supporting four major Hadoop releases - 0.20, 0.23, 1.0 and 2.0. We have installed Hadoop 2.2.0 and it is compatible with sqoop 1.4.4.We are using a Linux environment Ubuntu 12.04 to install and run sqoop. The basic familiarity with the purpose and operation of Hadoop is required to use this product.


\section{Installation}
\label{document:id14}
To install the sqoop 1.4.4 we followed the given sequence of steps :
\begin{enumerate}
\item {} 
Download the sqoop-1.4.4.bin\_hadoop-1.0.0.tar.gz  file from the link

\href{http://www.apache.org/dyn/closer.cgi/sqoop/1.4.4}{http://www.apache.org/dyn/closer.cgi/sqoop/1.4.4}

then click on this link:
\href{http://mirror.sdunix.com/apache/sqoop/1.4.4}{http://mirror.sdunix.com/apache/sqoop/1.4.4}

and download this zip:

\emph{sqoop-1.4.4.bin\_\_hadoop-1.0.0.tar.gz}

\item {} 
Change the directory to downloads using this command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  cd Downloads
\end{Verbatim}

\item {} 
Unzip the tar

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo tar \PYGZhy{}zxvf sqoop\PYGZhy{}1.4.4.bin\PYGZus{}\PYGZus{}hadoop\PYGZhy{}1.0.0.tar.gz
\end{Verbatim}

\item {} 
Move sqoop-1.4.4.bin hadoop1.0.0 to sqoop using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo mv sqoop 1.4.4.bin\PYGZus{}\PYGZus{}hadoop\PYGZhy{}1.0.0 /usr/local/sqoop
\end{Verbatim}

\item {} 
Create a directory sqoop in usr/lib using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo mkdir /usr/lib/sqoop
\end{Verbatim}

\item {} 
Go to the zipped folder sqoop-1.4.4.bin\_hadoop-1.0.0 and run the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}sudo mv ./* /usr/lib/sqoop
\end{Verbatim}

\item {} 
Go to root directory using cd command

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}\$  cd

\item {} 
Open bashrc file using

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}\$  sudo gedit \textasciitilde{}/.bashrc

\item {} 
Add the following lines

\begin{Verbatim}[commandchars=\\\{\}]
export SQOOP\PYGZus{}HOME=/usr/lib/sqoop
export PATH=\PYGZdl{}PATH:\PYGZdl{}SQOOP\PYGZus{}HOME/bin
\end{Verbatim}

\item {} 
To check if the sqoop has been installed  successfully type the command

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}\$ sqoop version

\end{enumerate}


\section{IMPORTING DATA FROM HADOOP TO MYSQL}
\label{document:importing-data-from-hadoop-to-mysql}
Run the command

\begin{Verbatim}[commandchars=\\\{\}]
sudo apt\PYGZhy{}get install mysql\PYGZhy{}server
\end{Verbatim}

and give appropriate username and password.
\begin{enumerate}
\item {} 
Download mysql-connector-java-5.1.28-bin.jar and move to /usr/lib/sqoop/lib using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo cp mysql\PYGZhy{}connnectpr\PYGZhy{}java\PYGZhy{}5.1.28\PYGZhy{}bin.jar
/usr/lib/sqoop/lib/
\end{Verbatim}

\item {} 
Login to mysql using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}   mysql \PYGZhy{}u root \PYGZhy{}p
\end{Verbatim}

\item {} 
Login to secure shell using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  ssh localhost
\end{Verbatim}

\item {} 
Start hadoop using the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  bin/hadoop start\PYGZhy{}all.sh
\end{Verbatim}

\item {} 
Run the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sqoop import \PYGZhy{}connect
jdbc:mysql://localhost:3306/sqoop \PYGZhy{}username root
\PYGZhy{}pasword abc \PYGZhy{}table employees \PYGZhy{}m
\end{Verbatim}

\end{enumerate}

This command imports the employees table from the sqoop directory of myql to hdfs.
\begin{enumerate}
\item {} 
Do check if the hadoop is in safe mode using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}hadoop dfsadmin \PYGZhy{}safemode get
\end{Verbatim}

\end{enumerate}

If you are getting safemode is on, run the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}hadoop dfsadmin \PYGZhy{}safemode leave
\end{Verbatim}

and again run the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}hadoop dfsadmin \PYGZhy{}safemode get
\end{Verbatim}

and confirm that you are getting safemode is off.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Do make sure that haoop is running before performing the import action.

\end{enumerate}


\chapter{EdX Description}
\label{document:edx-description}
EdX is a massive open online course (MOOC) destination site and online learning platform that is open sourced, founded by the Massachusetts Institute of Technology and Harvard University.

What we have done is analyze the log files generated by the student activities in the ITS (Intelligent tutoring system) i.e, the edX course site. The ultimate goal of our project is to determine that how many students enrolled in a course are actually interested in learning the course. If there are such students who are not interested in actually learning the course, then what are the activities they divulge into as an alternative in order to attain the course certificate. Such off-task behaviours have been termed as students `gaming' the system. Determination of such students will help us in the future to intervene the learning process, so as to prevent such students from gaming and help them in learning the course.

For this purpose, we are provided with the edX database which has the following tables (database `edxapp') :
\begin{itemize}
\item {} 
assessment\_assessment

\item {} 
assessment\_assessmentfeedback

\item {} 
assessment\_assessmentfeedback\_assessments

\item {} 
assessment\_assessmentfeedback\_options

\item {} 
assessment\_assessmentfeedbackoption

\item {} 
assessment\_assessmentpart

\item {} 
assessment\_criterion

\item {} 
assessment\_criterionoption

\item {} 
assessment\_peerworkflow

\item {} 
assessment\_peerworkflowitem

\item {} 
assessment\_rubric

\item {} 
auth\_group

\item {} 
auth\_group\_permissions

\item {} 
auth\_permission

\item {} 
auth\_registration

\item {} 
auth\_user

\item {} 
auth\_user\_groups

\item {} 
auth\_user\_user\_permissions

\item {} 
auth\_userprofile

\item {} 
bulk\_email\_courseauthorization

\item {} 
bulk\_email\_courseemail

\item {} 
bulk\_email\_courseemailtemplate

\item {} 
bulk\_email\_optout

\item {} 
celery\_taskmeta

\item {} 
celery\_tasksetmeta

\item {} 
certificates\_certificatewhitelist

\item {} 
certificates\_generatedcertificate

\item {} 
circuit\_servercircuit

\item {} 
course\_creators\_coursecreator

\item {} 
course\_groups\_courseusergroup

\item {} 
course\_groups\_courseusergroup\_users

\item {} 
course\_modes\_coursemode

\item {} 
courseware\_offlinecomputedgrade

\item {} 
courseware\_offlinecomputedgradelog

\item {} 
courseware\_studentmodule

\end{itemize}

Out of these tables, the tables we used for analysis were auth\_user, auth\_userprofile and courseware\_studentmodule. The rest tables being empty, we could not process much information from them.


\section{auth\_user}
\label{document:auth-user}
This table contains the following information of every user who has registered in the edx database :
\begin{itemize}
\item {} 
id

\item {} 
username

\item {} 
first\_name

\item {} 
last\_name

\item {} 
email

\item {} 
password

\item {} 
is\_staff

\item {} 
is\_active

\item {} 
is\_superuser

\item {} 
last\_login

\item {} 
last\_joined

\end{itemize}

The columns is\_staff, is\_active and is\_superuser are all binary valued and meanings can be easily predicted from ther names. The `is\_active' column holds the value `1' for those users who have confirmed their registration by clicking on the activation link sent to them on their mail.


\section{auth\_userprofile}
\label{document:auth-userprofile}
This table holds the demographic data collected from a student during the registration process and contains the following columns :
\begin{itemize}
\item {} 
id

\item {} 
user\_id

\item {} 
name

\item {} 
language

\item {} 
location

\item {} 
meta

\item {} 
courseware

\item {} 
gender

\item {} 
mailing\_address

\item {} 
year\_of\_birth

\item {} 
level\_of\_education

\item {} 
goals

\item {} 
allow\_certificate

\item {} 
country

\item {} 
city

\end{itemize}


\section{courseware\_studentmodule}
\label{document:courseware-studentmodule}
This table holds the courseware state of every student and has the following columns:
\begin{itemize}
\item {} 
id

\item {} 
module\_type

\item {} 
module\_id

\item {} 
student\_id

\item {} 
state

\item {} 
grade

\item {} 
created

\item {} 
modified

\item {} 
max\_grade

\item {} 
done

\item {} 
course\_id

\end{itemize}

Using these tables we have come up with some hive queries(described in the following section) for the analytics part.


\section{Research paper study}
\label{document:research-paper-study}
After all the installation procedures, we studied the research paper, “Detecting Student Misuse of Intelligent Tutoring Systems” authored by Ryan Shaun Baker, Albert T. Corbett, Kenneth R. Koedinger and gave a presentation on it. Their study says that students who are averted to such ‘gaming the system behaviour’ (behavior aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software’s feedback and help) learn 2/3rds as much as similar students who do not engage in such behaviors. They came up with a machine-learned latent response model that can identify whether a student is gaming the system or not. Based on these predictions, the tutor can be re-designed for such students and make their learning process effective.

Baker and his colleagues found that a student’s frequency of gaming was strongly negatively correlated with learning. According to them, understanding why students game the system will be essential in deciding how the system should respond. Ultimately, though, whatever remediation approach is chosen, it is likely to have costs as well as benefits. For instance, preventive approaches, such as changing interface widgets to make them more difficult to game or delaying successive levels of help to prevent rapid-fire usage, may reduce gaming, but at the cost of making the tutor more frustrating and less time-efficient for other students. Since many students use help effectively and seldom or never game the system, the costs of using such an approach indiscriminately may be higher than the rewards. Whichever approach we take to remediating gaming the system, the success of that approach is likely to depend on accurately and automatically detecting which students are gaming the system and which are not.

The LRM they suggested, takes 24 features as input or data source and also the predetermined value of the student ‘gaming or not’ of a training set of 70 students. Then it uses forward selection for model selection and then finally implements iterative gradient descent to find the best model parameters. The best-fitting model had 4 parameters, and no model considered had more than 6 parameters. They also used a cross-validation techninque, LOOCV (Leave One Out Cross Validation). Finally with the ROC (Receiver Operating Characteristic) curve, they classified the student as gaming or not gaming. On this result, they applied the interventions in the ITS.


\chapter{\textbf{Log Parsing}}
\label{document:log-parsing}
Log files provides information about the event data that is delivered in data packages. Events are emitted by the server or the browser to capture information about interactions with the courseware and the Instructor Dashboard in the LMS, and are stored in JSON documents. In the data package, event data is delivered in a log file.The JSON documents that include event data are delivered in a machine-readable format that is difficult to read . A sample is the following :-

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZdq{}agent\PYGZdq{}: \PYGZdq{}Mozilla/5.0 (X11; Linux x86\PYGZus{}64) AppleWebKit/537.36
(KHTML, like Gecko)Chrome/30.0.1599.101 Safari/537.36\PYGZdq{}, \PYGZdq{}context\PYGZdq{}:
\PYGZob{}\PYGZdq{}course\PYGZus{}id\PYGZdq{}: \PYGZdq{}edx/AN101/2014\PYGZus{}T1\PYGZdq{},\PYGZdq{}module\PYGZdq{}: \PYGZob{}\PYGZdq{}display\PYGZus{}name\PYGZdq{}:
\PYGZdq{}Multiple Choice Questions\PYGZdq{}\PYGZcb{}, \PYGZdq{}org\PYGZus{}id\PYGZdq{}:\PYGZdq{}edx\PYGZdq{}, \PYGZdq{}user\PYGZus{}id\PYGZdq{}:
9999999\PYGZcb{}, \PYGZdq{}event\PYGZdq{}: \PYGZob{}\PYGZdq{}answers\PYGZdq{}: \PYGZob{}\PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954
cca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}:\PYGZdq{}yellow\PYGZdq{}, \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0
effb954cca4759994f1ac9e9434bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}: [\PYGZdq{}choice\PYGZus{}0\PYGZdq{}, \PYGZdq{}choice\PYGZus{}2\PYGZdq{}]\PYGZcb{},
\PYGZdq{}attempts\PYGZdq{}: 1, \PYGZdq{}correct\PYGZus{}map\PYGZdq{}: \PYGZob{}\PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954c
ca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}:\PYGZob{}\PYGZdq{}correctness\PYGZdq{}: \PYGZdq{}incorrect\PYGZdq{}, \PYGZdq{}hint\PYGZdq{}:
\PYGZdq{}\PYGZdq{}, \PYGZdq{}hintmode\PYGZdq{}: null, \PYGZdq{}msg\PYGZdq{}: \PYGZdq{}\PYGZdq{}, \PYGZdq{}npoints\PYGZdq{}: null,\PYGZdq{}queuestate\PYGZdq{}: null\PYGZcb{}
, \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}:
\PYGZob{}\PYGZdq{}correctness\PYGZdq{}: \PYGZdq{}correct\PYGZdq{}, \PYGZdq{}hint\PYGZdq{}: \PYGZdq{}\PYGZdq{}, \PYGZdq{}hintmode\PYGZdq{}: null, \PYGZdq{}msg\PYGZdq{}: \PYGZdq{}\PYGZdq{},
\PYGZdq{}npoints\PYGZdq{}: null,\PYGZdq{}queuestate\PYGZdq{}: null\PYGZcb{}\PYGZcb{}, \PYGZdq{}grade\PYGZdq{}: 2, \PYGZdq{}max\PYGZus{}grade\PYGZdq{}: 3,
\PYGZdq{}problem\PYGZus{}id\PYGZdq{}: \PYGZdq{}i4x://edx/AN101/problem/a0effb954cca4759994f1ac9e94
34bf4\PYGZdq{}, \PYGZdq{}state\PYGZdq{}: \PYGZob{}\PYGZdq{}correct\PYGZus{}map\PYGZdq{}: \PYGZob{}\PYGZcb{}, \PYGZdq{}done\PYGZdq{}: null, \PYGZdq{}input\PYGZus{}state\PYGZdq{}:
\PYGZob{}\PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}: \PYGZob{}\PYGZcb{},
\PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}: \PYGZob{}\PYGZcb{}\PYGZcb{},
\PYGZdq{}seed\PYGZdq{}: 1, \PYGZdq{}student\PYGZus{}answers\PYGZdq{}: \PYGZob{}\PYGZcb{}\PYGZcb{}, \PYGZdq{}submission\PYGZdq{}:\PYGZob{}\PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}
problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}: \PYGZob{}\PYGZdq{}answer\PYGZdq{}: \PYGZdq{}yellow\PYGZdq{},
\PYGZdq{}correct\PYGZdq{}:false, \PYGZdq{}input\PYGZus{}type\PYGZdq{}: \PYGZdq{}optioninput\PYGZdq{}, \PYGZdq{}question\PYGZdq{}: \PYGZdq{}What color
is the open ocean on a sunny day?\PYGZdq{},\PYGZdq{}response\PYGZus{}type\PYGZdq{}: \PYGZdq{}optionresponse\PYGZdq{},
\PYGZdq{}variant\PYGZdq{}: \PYGZdq{}\PYGZdq{}\PYGZcb{}, \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e943
4bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}: \PYGZob{}\PYGZdq{}answer\PYGZdq{}: [\PYGZdq{}a piano\PYGZdq{}, \PYGZdq{}a guitar\PYGZdq{}], \PYGZdq{}correct\PYGZdq{}: true,
\PYGZdq{}input\PYGZus{}type\PYGZdq{}: \PYGZdq{}checkboxgroup\PYGZdq{}, \PYGZdq{}question\PYGZdq{}: \PYGZdq{}Which of the following
are musical instruments?\PYGZdq{},\PYGZdq{}response\PYGZus{}type\PYGZdq{}: \PYGZdq{}choiceresponse\PYGZdq{}, \PYGZdq{}variant\PYGZdq{}:
\PYGZdq{}\PYGZdq{}\PYGZcb{}\PYGZcb{}, \PYGZdq{}success\PYGZdq{}: \PYGZdq{}incorrect\PYGZdq{}\PYGZcb{}, \PYGZdq{}event\PYGZus{}source\PYGZdq{}:\PYGZdq{}server\PYGZdq{}, \PYGZdq{}event\PYGZus{}type\PYGZdq{}:
\PYGZdq{}problem\PYGZus{}check\PYGZdq{}, \PYGZdq{}host\PYGZdq{}: \PYGZdq{}precise64\PYGZdq{}, \PYGZdq{}ip\PYGZdq{}: \PYGZdq{}NN.N.N.N\PYGZdq{}, \PYGZdq{}page\PYGZdq{}: \PYGZdq{}x\PYGZus{}module\PYGZdq{},
\PYGZdq{}time\PYGZdq{}: 2014\PYGZhy{}03\PYGZhy{}03T16:19:05.584523+00:00\PYGZdq{}, \PYGZdq{}username\PYGZdq{}: \PYGZdq{}AAAAAAAAAA\PYGZdq{}\PYGZcb{}
\end{Verbatim}

We can use pretty print( by jq `.' command) to see this information in a readable format which is shown as below :-

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZob{}
   \PYGZdq{}agent\PYGZdq{}: \PYGZdq{}Mozilla/5.0 (X11; Linux x86\PYGZus{}64) AppleWebKit/537.36
   (KHTML, like Gecko)
   Chrome/30.0.1599.101 Safari/537.36\PYGZdq{},
   \PYGZdq{}context\PYGZdq{}: \PYGZob{}
       \PYGZdq{}course\PYGZus{}id\PYGZdq{}: \PYGZdq{}edx/AN101/2014\PYGZus{}T1\PYGZdq{},
       \PYGZdq{}module\PYGZdq{}: \PYGZob{}
           \PYGZdq{}display\PYGZus{}name\PYGZdq{}: \PYGZdq{}Multiple Choice Questions\PYGZdq{}
       \PYGZcb{},
       \PYGZdq{}org\PYGZus{}id\PYGZdq{}: \PYGZdq{}edx\PYGZdq{},
       \PYGZdq{}user\PYGZus{}id\PYGZdq{}: 9999999
   \PYGZcb{},
   \PYGZdq{}event\PYGZdq{}: \PYGZob{}
       \PYGZdq{}answers\PYGZdq{}: \PYGZob{}
           \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}:\PYGZdq{}yellow\PYGZdq{},
           \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}: [
               \PYGZdq{}choice\PYGZus{}0\PYGZdq{},
               \PYGZdq{}choice\PYGZus{}2\PYGZdq{}
           ]
       \PYGZcb{},
       \PYGZdq{}attempts\PYGZdq{}: 1,
       \PYGZdq{}correct\PYGZus{}map\PYGZdq{}: \PYGZob{}
           \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}:\PYGZob{}
               \PYGZdq{}correctness\PYGZdq{}: \PYGZdq{}incorrect\PYGZdq{},
               \PYGZdq{}hint\PYGZdq{}: \PYGZdq{}\PYGZdq{},
               \PYGZdq{}hintmode\PYGZdq{}: null,
               \PYGZdq{}msg\PYGZdq{}: \PYGZdq{}\PYGZdq{},
               \PYGZdq{}npoints\PYGZdq{}: null,
               \PYGZdq{}queuestate\PYGZdq{}: null
           \PYGZcb{},
           \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}:\PYGZob{}
               \PYGZdq{}correctness\PYGZdq{}: \PYGZdq{}correct\PYGZdq{},
               \PYGZdq{}hint\PYGZdq{}: \PYGZdq{}\PYGZdq{},
               \PYGZdq{}hintmode\PYGZdq{}: null,
               \PYGZdq{}msg\PYGZdq{}: \PYGZdq{}\PYGZdq{},
               \PYGZdq{}npoints\PYGZdq{}: null,
               \PYGZdq{}queuestate\PYGZdq{}: null
           \PYGZcb{}
       \PYGZcb{},
       \PYGZdq{}grade\PYGZdq{}: 2,
       \PYGZdq{}max\PYGZus{}grade\PYGZdq{}: 3,
       \PYGZdq{}problem\PYGZus{}id\PYGZdq{}: \PYGZdq{}i4x://edx/AN101/problem/a0effb954cca4759994f1ac9e9434bf4\PYGZdq{},
       \PYGZdq{}state\PYGZdq{}: \PYGZob{}
           \PYGZdq{}correct\PYGZus{}map\PYGZdq{}: \PYGZob{}\PYGZcb{},
           \PYGZdq{}done\PYGZdq{}: null,
           \PYGZdq{}input\PYGZus{}state\PYGZdq{}: \PYGZob{}
               \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}:\PYGZob{}\PYGZcb{},
               \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}:\PYGZob{}\PYGZcb{}
           \PYGZcb{},
           \PYGZdq{}seed\PYGZdq{}: 1,
           \PYGZdq{}student\PYGZus{}answers\PYGZdq{}: \PYGZob{}\PYGZcb{}
       \PYGZcb{},
       \PYGZdq{}submission\PYGZdq{}: \PYGZob{}
           \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}2\PYGZus{}1\PYGZdq{}: \PYGZob{}
               \PYGZdq{}answer\PYGZdq{}: \PYGZdq{}yellow\PYGZdq{},
               \PYGZdq{}correct\PYGZdq{}: false,
               \PYGZdq{}input\PYGZus{}type\PYGZdq{}: \PYGZdq{}optioninput\PYGZdq{},
               \PYGZdq{}question\PYGZdq{}: \PYGZdq{}What color is the open ocean on a sunny day?\PYGZdq{},
               \PYGZdq{}response\PYGZus{}type\PYGZdq{}: \PYGZdq{}optionresponse\PYGZdq{},
               \PYGZdq{}variant\PYGZdq{}: \PYGZdq{}\PYGZdq{}
           \PYGZcb{},
           \PYGZdq{}i4x\PYGZhy{}edx\PYGZhy{}AN101\PYGZhy{}problem\PYGZhy{}a0effb954cca4759994f1ac9e9434bf4\PYGZus{}4\PYGZus{}1\PYGZdq{}: \PYGZob{}
               \PYGZdq{}answer\PYGZdq{}: [
                   \PYGZdq{}a piano\PYGZdq{},
                   \PYGZdq{}a guitar\PYGZdq{}
               ],
               \PYGZdq{}correct\PYGZdq{}: true,
               \PYGZdq{}input\PYGZus{}type\PYGZdq{}: \PYGZdq{}checkboxgroup\PYGZdq{},
               \PYGZdq{}question\PYGZdq{}: \PYGZdq{}Which of the following are musical instruments?\PYGZdq{},
               \PYGZdq{}response\PYGZus{}type\PYGZdq{}: \PYGZdq{}choiceresponse\PYGZdq{},
               \PYGZdq{}variant\PYGZdq{}: \PYGZdq{}\PYGZdq{}
           \PYGZcb{}
       \PYGZcb{},
       \PYGZdq{}success\PYGZdq{}: \PYGZdq{}incorrect\PYGZdq{}
   \PYGZcb{},
   \PYGZdq{}event\PYGZus{}source\PYGZdq{}: \PYGZdq{}server\PYGZdq{},
   \PYGZdq{}event\PYGZus{}type\PYGZdq{}: \PYGZdq{}problem\PYGZus{}check\PYGZdq{},
   \PYGZdq{}host\PYGZdq{}: \PYGZdq{}precise64\PYGZdq{},
   \PYGZdq{}ip\PYGZdq{}: \PYGZdq{}NN.N.N.N\PYGZdq{},
   \PYGZdq{}page\PYGZdq{}: \PYGZdq{}x\PYGZus{}module\PYGZdq{},
   \PYGZdq{}time\PYGZdq{}: \PYGZdq{}2014\PYGZhy{}03\PYGZhy{}03T16:19:05.584523+00:00\PYGZdq{},
   \PYGZdq{}username\PYGZdq{}: \PYGZdq{}AAAAAAAAAA\PYGZdq{}
\PYGZcb{}
\end{Verbatim}

The JSON object `event\_source' is the most important feture we are dealing with. All the entries in the tracking.log file get parsed into the log table of our database. There is an entry corresponding to every event generated in the log files. Different types of events are generated. Common fields of those events are as follows:-
\begin{quote}\begin{description}
\item[{Agent}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] Browser agent string of the user who triggered the event
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{Context}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] For all events, this field includes member fields that identify:
* The course\_id of the course that generated the event.
* The org\_id of the organization that lists the course.
* The user\_id of the individual who is performing the action.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{Event}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This field includes member fields that identify specifics of each triggered event.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{event\_source}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] Specifies whether the triggered event originated in the browser or on the server.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{event\_type}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] The type of event triggered.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{Host}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] The site visited by the user, for example, courses.edx.org.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{ip}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] IP address of the user who triggered the event.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{page}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] The `\$URL' of the page the user was visiting when the event was emitted.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{session}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This 32-character value is a key that identifies the user's session.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{time}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] Gives the UTC time at which the event was emitted in `YYYY-MM- DDThh:mm:ss.xxxxxx' format.
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{username}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] The username of the user who caused the event to be emitted. This string is empty for anonymous events, such as when the user is not logged in.
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] Each and every event has its own attributes. Various types of events generated which we have used are as follows:-
\end{DUlineblock}
\begin{itemize}
\item {} 
Problem\_check generated at two sources , browser and server.

\item {} 
Problem\_show

\item {} 
load\_video

\item {} 
play\_video

\item {} 
speed\_change\_video

\item {} 
seek\_video

\item {} 
pause\_video

\end{itemize}


\section{Procedure}
\label{document:procedure}
\begin{DUlineblock}{0em}
\item[] Entries of log files are parsed and then stored in database.
\item[] For each log entry genral data like ip address, event\_type, event\_source is stored in table log.
\item[] Data related to specific event\_type is stored in corresponding tables like play\_video, puase\_video.
\end{DUlineblock}

The entries of log files are parsed and then stored in database which has the following tables.
\begin{quote}\begin{description}
\item[{Log}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores the common log entries of all types of events and has the  folllowing fields:-
\end{DUlineblock}
\begin{itemize}
\item {} 
id

\item {} 
course\_id

\item {} 
org\_id

\item {} 
user\_id

\item {} 
event\_type

\item {} 
event\_type

\item {} 
event\_source

\item {} 
host

\item {} 
ip

\item {} 
page

\item {} 
time

\item {} 
username

\end{itemize}
\begin{quote}\begin{description}
\item[{load\_video}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is load\_video.Here the log\_id corresponds to the id of log table.This table has   the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\item {} 
code

\end{itemize}
\begin{quote}\begin{description}
\item[{pause\_video}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is pause\_video.  Here the log\_id                 corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\item {} 
code

\item {} 
current\_time

\end{itemize}
\begin{quote}\begin{description}
\item[{play\_video}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is play\_video.  Here the log\_id                 corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\item {} 
code

\item {} 
current\_time

\end{itemize}
\begin{quote}\begin{description}
\item[{problem\_check\_browser}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is problem\_check and event source is browser.  Here the log\_id    corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\end{itemize}
\begin{quote}\begin{description}
\item[{problem\_check\_server}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is problem\_check and event source is server.  Here the log\_id    corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
problem\_id

\item {} 
log\_id

\item {} 
problem\_id

\item {} 
hint

\item {} 
hintmode

\item {} 
correctness

\item {} 
response\_type

\item {} 
input\_type

\end{itemize}
\begin{quote}\begin{description}
\item[{problem\_show}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is problem\_show.  Here the log\_id                 corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\item {} 
problem\_id

\end{itemize}
\begin{quote}\begin{description}
\item[{seek\_video}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is seek\_video.  Here the log\_id                 corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\item {} 
code

\item {} 
old\_time

\item {} 
new\_time

\end{itemize}
\begin{quote}\begin{description}
\item[{speed\_change\_video}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is speed\_change\_video.  Here the log\_id   corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\item {} 
code

\item {} 
current\_time

\item {} 
old\_speed

\item {} 
new\_speed

\end{itemize}
\begin{quote}\begin{description}
\item[{load\_video}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores details of log entries whose event\_type is load\_video.  Here the log\_id corresponds  to the id of log table.This table has the following columns:-
\end{DUlineblock}
\begin{itemize}
\item {} 
log\_id

\item {} 
code

\end{itemize}
\begin{quote}\begin{description}
\item[{status}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This table stores the attributes related to the log files and the files which have been archived.Some examples are size,line read , date modified.
\end{DUlineblock}
\begin{itemize}
\item {} 
name

\item {} 
int

\end{itemize}

\emph{Database Schema}
\begin{figure}[htbp]
\centering

\scalebox{0.600000}{\includegraphics{db_schema.png}}
\end{figure}

Following jar files are used in the the java program to parse the log files:-
\begin{itemize}
\item {} 
java-json.jar

\item {} 
apache-logging-log4j.jar

\item {} 
commons-httpclient-3.0.1.jar

\item {} 
commons-logging-1.1.3.jar

\item {} 
hadoop-core-1.2.0.jar

\item {} 
hive-cli-0.13.0.jar

\item {} 
hive-common-0.13.0.jar

\item {} 
hive-exec-0.13.0.jar

\item {} 
hive-jdbc-0.13.0.jar

\item {} 
hive-metastore-0.13.0.jar

\item {} 
hive-service-0.13.0.jar

\item {} 
hive.txt

\item {} 
libfb303-0.9.0.jar

\item {} 
libthrift-0.9.0.jar

\item {} 
log4j-1.2.16.jar

\item {} 
slf4j-api-1.7.7.jar

\item {} 
slf4j-jdk14-1.7.7.jar

\end{itemize}

Let the old\_size be the size of the file which has been already parsed and whose value has been stored in the status table. Let new\_size be the size of the log file at present.
There are three cases for the log files which are as folllows:-

\begin{DUlineblock}{0em}
\item[] 1. old\_size \textless{} new\_size
\end{DUlineblock}

It means that new entries has been added to the log files .
The value of number of lines that have been parsed is stored in status table .That value is retrieved and those many lines skipped.Rest of the lines are parsed.Log entries are in the form of json .Folllowing is a snippet for this case:-

\begin{Verbatim}[commandchars=\\\{\}]
public class LineParser \PYGZob{}
public int parseline(String line)
\PYGZob{}
   int success=0;
   Database db = new Database();
   JSONObject rootObject;
   try \PYGZob{}
       //parsing json object in the form of string passed by edxparser
       rootObject = new JSONObject(line);

       //creating object of log class to hold the data parsed
       //from the json objects.
       Log log = new Log();

   //parsing nested object context
            JSONObject context = rootObject.getJSONObject(\PYGZdq{}context\PYGZdq{});
            log.setCourse\PYGZus{}id(context.get(\PYGZdq{}course\PYGZus{}id\PYGZdq{}).toString();
            log.setOrg\PYGZus{}id(context.get(\PYGZdq{}org\PYGZus{}id\PYGZdq{}).toString);
            try
            \PYGZob{}
                    log.setUser\PYGZus{}id((Integer)context.get(\PYGZdq{}user\PYGZus{}id\PYGZdq{}));
            \PYGZcb{}
            catch(Exception e)
            \PYGZob{}
                    log.setUser\PYGZus{}id(0);
            \PYGZcb{}
            try
            \PYGZob{}
                //parsing the nested json object module
                JSONObject module = context.getJSONObject(\PYGZdq{}module\PYGZdq{});
                log.setModule(module.get(\PYGZdq{}display\PYGZus{}name\PYGZdq{}).toString());
            \PYGZcb{}
            catch (Exception e)
            \PYGZob{}
                    log.setModule(\PYGZdq{}\PYGZdq{});
            \PYGZcb{}
            log.setEvent\PYGZus{}source(rootObject.get(\PYGZdq{}event\PYGZus{}source\PYGZdq{}).toString());
            log.setEvent\PYGZus{}type(rootObject.get(\PYGZdq{}event\PYGZus{}type\PYGZdq{}).toString());
            log.setHost(rootObject.get(\PYGZdq{}host\PYGZdq{}).toString());
            log.setIp(rootObject.get(\PYGZdq{}ip\PYGZdq{}).toString());
            log.setPage(rootObject.get(\PYGZdq{}page\PYGZdq{}).toString());

    //converting time into desired format
            String time = rootObject.get(\PYGZdq{}time\PYGZdq{}).toString();
            String time2=time.substring(0, 10);
            time2=time2.concat(\PYGZdq{} \PYGZdq{});
            time2=time2.concat(time.substring(11,19));
            log.setTime(time2);
            log.setUsername(rootObject.get(\PYGZdq{}username\PYGZdq{}).toString());
            log.setEvent(rootObject.get(\PYGZdq{}event\PYGZdq{}).toString());
            try
            \PYGZob{}
                    log.setSession(rootObject.get(\PYGZdq{}session\PYGZdq{}).toString());
            \PYGZcb{}
            catch(Exception e)
            \PYGZob{}
                    log.setSession(\PYGZdq{}\PYGZdq{});
            \PYGZcb{}

    //passing log object to the function insertlogdate to be inserted
    //into the database.
            success = db.insertlogdata(log);
    \PYGZcb{}
    catch (JSONException e1)
    \PYGZob{}
      e1.printStackTrace();
    \PYGZcb{}
return success;
\PYGZcb{}
\PYGZcb{}
\end{Verbatim}

\begin{DUlineblock}{0em}
\item[] 2. old\_size=new\_size
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] The log file has not changed and no new entries added.
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 3. old\_size \textgreater{}new\_size
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] This means that the log file has been archived.So all those file whose modification time is greater than the time stored in the status table, which is the modification time of the log file just processed, are retrieved and extracted.
\item[] It is possible that the current file which program was parsing is modified i.e., new log entries are added to the file and is now archived. So, we need to find the archived file corresponding to the file which was being parsed and parse and process if any new entries were added to the file.
\item[] The file whose modification date is oldest among those files is the file which was archived while program was parsing it. Those new entries which were added are parsed with the help of values such as linesparsed and size of file stored in the status table.Rest of the files are processed from beginning to the end.
\end{DUlineblock}

Following is a code snippet of the following case:

\begin{Verbatim}[commandchars=\\\{\}]
public class  \PYGZob{}
    public void handlefile()
    \PYGZob{}
            //directory containing log data
            File mydirectory = new File(\PYGZdq{}/home/sachin/workspace/json
            /src/json/log\PYGZdq{});

            System.out.println(mydirectory.isDirectory());

    //creating the array of files to store list of all files in the
    //log directory.
            File names[] = mydirectory.listFiles();
            Database db = new Database();

            //getting filelastmodified date
            long lastmdate=db.getfilelastmodified();

            //creating arraylist to store new tar.gz files
            ArrayList\PYGZlt{}File\PYGZgt{} left = new ArrayList\PYGZlt{}File\PYGZgt{}();

            //adding new files into the arraylist
            for(int i=0;i\PYGZlt{}names.length;i++) \PYGZob{}
    if(names[i].lastModified()\PYGZgt{}lastmdate\PYGZam{}\PYGZam{}names[i].toString().matches(\PYGZdq{}.*gz\PYGZdl{}\PYGZdq{}))
    \PYGZob{}
        left.add(names[i]);
        System.out.println(\PYGZdq{}got match: \PYGZdq{}+names[i].lastModified()+\PYGZdq{} \PYGZdq{}+names[i]);
    \PYGZcb{}
            \PYGZcb{}

    //creating the table to sort content according to the modification date
            long table[][] = new long[left.size()][2];

            //intializing two dimensional array
            for(int i=0;i\PYGZlt{}left.size();i++)
            \PYGZob{}
                    table[i][0] = i;
                    table[i][1] = left.get(i).lastModified();
                    System.out.println(table[i][0]+\PYGZdq{} \PYGZdq{}+table[i][1]);
            \PYGZcb{}

            //sorting according to the date
            for (int c = 0; c \PYGZlt{}  left.size(); c++)
            \PYGZob{}
               for (int d = 0; d \PYGZlt{} left.size() \PYGZhy{} c \PYGZhy{} 1; d++)
               \PYGZob{}
                       if(table[d][1] \PYGZgt{} table[d+1][1])
                       \PYGZob{}
                               long swap = table[d][0];
                               table[d][0]=table[d+1][0];
                               table[d+1][0]=swap;
                               swap = table[d][1];
                               table[d][1]=table[d+1][1];
                               table[d+1][1]=swap;
                       \PYGZcb{}
               \PYGZcb{}
            \PYGZcb{}
            InputStream is=null;
            boolean first=true;
            String line;

    //creating object of lineparser to parse the json object from archived
    //files
            LineParser lp = new LineParser();
            for(int i=0;i\PYGZlt{}left.size();i++)
            \PYGZob{}
    try
    \PYGZob{}

    //first represents the oldest file in the log folder which is not processed.
    if(first)
    \PYGZob{}

        System.out.println(\PYGZdq{}started reading file \PYGZdq{}+left.get((int)table[i][0]));
        int linenum = db.getlinenum();
        System.out.println(\PYGZdq{}line count in file\PYGZdq{}+linenum);
        is=new GZIPInputStream(new FileInputStream(left.get((int)table[i][0])));
        BufferedReader buffered = new BufferedReader(new InputStreamReader(is));
        int j=0;
        //skipping already parsed lines
        while(j\PYGZlt{}linenum)
        \PYGZob{}
            if((line=buffered.readLine())!=null)
            \PYGZob{}
                j++;
                System.out.println(\PYGZdq{}The value of the j is \PYGZdq{}+j);
            \PYGZcb{}
        \PYGZcb{}
        while((line=buffered.readLine())!=null)
        \PYGZob{}
            System.out.println(line);
            if(line.startsWith(\PYGZdq{}\PYGZob{}\PYGZdq{}))
            \PYGZob{}
                lp.parseline(line);
            \PYGZcb{}
        \PYGZcb{}
        buffered.close();

        //setting value of the first to false to indicate oldest file has been
        //already parsed
        first=false;
        db.setfilelastmodified(left.get((int) table[i][0]).lastModified());
    \PYGZcb{}
    else
    \PYGZob{}

        System.out.println(\PYGZdq{}started reading file \PYGZdq{}+left.get((int)table[i][0]));
        is=new GZIPInputStream(new FileInputStream(left.get((int)table[i][0])));
        Reader decoder = new InputStreamReader(is, \PYGZdq{}UTF\PYGZhy{}8\PYGZdq{});
        BufferedReader buffered = new BufferedReader(decoder);
        while((line=buffered.readLine())!=null)
        \PYGZob{}
            if(line.startsWith(\PYGZdq{}\PYGZob{}\PYGZdq{}))
            \PYGZob{}
                lp.parseline(line);
            \PYGZcb{}
            System.out.println(line);
        \PYGZcb{}
        buffered.close();
        db.setfilelastmodified(left.get((int) table[i][0]).lastModified());
    \PYGZcb{}
    \PYGZcb{}
    catch(Exception e)
    \PYGZob{}
        System.out.println(\PYGZdq{}error while reading tar.gz files\PYGZdq{}+left.get
        ((int) table[i][0]));
    \PYGZcb{}
            \PYGZcb{}

    //resetting the entries in status table to the value zero.
            db.setsize(0);
            db.insertlinenum(0);
    \PYGZcb{}
\PYGZcb{}
\end{Verbatim}

And the flowchart for the same is:
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{log.png}}
\end{figure}

\begin{DUlineblock}{0em}
\item[] Diagram 1 represent program parsing the tracking.log file and storing the value of number of lines parsed into the table status.
\item[] Diagram 2 represents the state when the file which program was previously parsing is now archived, it is possible that few log entries were added to this file and before the program parse those log entries tracking.log is archived. It may happen that the newly genrated tracking.log file is also archived before the program starts parsing the tracking.log file in this way, log directory will contain the two archived files one which is partially processed and the one which is not at all processed.
\end{DUlineblock}
\begin{figure}[htbp]
\centering

\scalebox{1.000000}{\includegraphics{diagram.png}}
\end{figure}

After the classification has been done, queries were written on the tables to extract the features required for implementing machine learning.

The database schema above defined was first made in mysql. Then it was imported into hive by sqoop. But later on, we came up with a method by which we could directly connect with hive from the java program.


\chapter{Connection of Hive with Java}
\label{document:connection-of-hive-with-java}
\begin{DUlineblock}{0em}
\item[] Since we are using JAVA programs for feature extraction on the tables of HIVE , we need to connect to HIVE in JAVA program .We can run Hive queries from a Java Database Connectivity (JDBC)  Connectivity (ODBC) application using  the Hive JDBC  driver. The Hive JDBC driver allows you to access Hive from a Java program  that uses JDBC to communicate with database products.
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] Hive provides a Type 4 (pure Java) JDBC driver, defined in the class an application that can connect to a Hive server using the Hive JDBC driver  org.apache.hadoop.hive.jdbc.HiveDriver. When configured with a JDBC URI of the form jdbc:hive://host:port/dbname, a Java application can connect to a Hive server running at the specifed host and port. The driver makes calls to an interface implemented by the Hive Thrift Client using the Java Thrift bindings. Before connecting to HIVE in a JAVA program , we need to run hiveserver on a particular port.The default port number for hiveserver is 10000.We need to mention this port number in our JAVA program.
\end{DUlineblock}
\begin{figure}[htbp]
\centering

\scalebox{0.600000}{\includegraphics{pik1.jpg}}
\end{figure}

The project which is connecting to hive should have the following jar files incuded:
\begin{itemize}
\item {} 
apache-logging-log4j.jar

\item {} 
commons-httpclient-3.0.1.jar

\item {} 
commons-logging-1.1.3.jar

\item {} 
hadoop-core-1.2.0.jar

\item {} 
hive-cli-0.13.0.jar

\item {} 
hive-common-0.13.0.jar

\item {} 
hive-exec-0.13.0.jar

\item {} 
hive-jdbc-0.13.0.jar

\item {} 
hive-metastore-0.13.0.jar

\item {} 
hive-service-0.13.0.jar

\item {} 
hive.txt

\item {} 
libfb303-0.9.0.jar

\item {} 
libthrift-0.9.0.jar

\item {} 
log4j-1.2.16.jar

\item {} 
slf4j-api-1.7.7.jar

\item {} 
slf4j-jdk14-1.7.7.jar

\end{itemize}

The Connect class code snippet looks like :

\begin{Verbatim}[commandchars=\\\{\}]
Class.forName(\PYGZdq{}org.apache.hadoop.hive.jdbc.HiveDriver\PYGZdq{});
Connection connect = DriverManager.getConnection
(\PYGZdq{}jdbc:hive://localhost:10000/exptnew\PYGZdq{}, \PYGZdq{}\PYGZdq{}, \PYGZdq{}\PYGZdq{});
System.out.println(\PYGZdq{}Connected successfully\PYGZdq{});
return(connect);
\end{Verbatim}


\chapter{Feature Extraction}
\label{document:feature-extraction}
In order to determine if a student is gaming the system or not, we have written some queries on the parsed log data (the database we created for the entries from the log file). The features we came up with are :-

\begin{DUlineblock}{0em}
\item[] 1. To calculate the minimum number of attempts a student takes in correctly answering a question
\item[] 2. To calculate the amount of tme a student is seeking a video
\item[] 3. To calculate the difficulty level of each question in every course
\item[] 4. To calculate the activity level of a student per day.
\end{DUlineblock}

In order to find the difficulty level of the question, it is required to know in how many minimum number of attempts a student was able to answer the question correctly. Edx allows a user to answer a question any number of times even after the student has already given the correct answer. This is why we need to consider only the first attempt in which the student correctly answered the question. As analyzing the attempts after the user has already correctly answered the question is futile because student already knew the answer and those extra attempts will only mean revision for the student or that the student is just playing around with the system.

The java class written for this purpose is Feature\_no\_of\_attempts. The following snippet fulfills the above described functionality :-

\begin{Verbatim}[commandchars=\\\{\}]
//storing data into temporary table
//start and end are obtained form status and log table
//respectively.
statement.executeQuery(\PYGZdq{}insert overwrite table temp select
problem\PYGZus{}id,username,min(attempts)
from problem\PYGZus{}check\PYGZus{}server where log\PYGZus{}id\PYGZgt{}\PYGZdq{}+start+\PYGZdq{} and
log\PYGZus{}id\PYGZlt{}=\PYGZdq{}+end+\PYGZdq{} and correctness=\PYGZsq{}correct\PYGZsq{}
group by problem\PYGZus{}id,username\PYGZdq{});

 System.out.println(\PYGZdq{}first and last \PYGZdq{}+start+\PYGZdq{} \PYGZdq{}+end);
 //inserting dummy entries into the table attempts
 statement.executeQuery(\PYGZdq{}insert into table attempts select
  p.problem\PYGZus{}id,p.username,0
 from temp p where not exists(select * from attempts where
 problem\PYGZus{}id=p.problem\PYGZus{}id and
 username=p.username)\PYGZdq{});
 resultset=statement.executeQuery(\PYGZdq{}select * from temp\PYGZdq{});
 while(resultset.next())
 \PYGZob{}
     a=resultset.getString(1);
     b=resultset.getString(2);
     c=resultset.getInt(3);
     //modifying the dummy entries with the help of temporary
      //table
     statement.executeQuery(\PYGZdq{}insert overwrite table attempts
     select problem\PYGZus{}id,username,
     case when problem\PYGZus{}id=\PYGZsq{}\PYGZdq{}+a+\PYGZdq{}\PYGZsq{} and username=\PYGZsq{}\PYGZdq{}+b+\PYGZdq{}\PYGZsq{} then
     \PYGZdq{}+c+\PYGZdq{} else attempts end as
     attempts from attempts\PYGZdq{});
     System.out.println(resultset.getString(1)+resultset.
     getString(2)+resultset
     .getString(3));
 \PYGZcb{}
\end{Verbatim}

Finding the number of minimum attempts per user per question has been done in three steps. This feature has been extracted using incremental approach. First step is to identify the new users from newly generated log entries and store them into temporary table along with the minimum number of attempts per question in the corresponding log entries. Second step is to insert dummy entries into the table attempts for the user not already present in the attempts table. Third step is to insert the values extracted in the temporary table into the attempts table.

\emph{Image for Minimum no. of attemps}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{attempts.png}}
\end{figure}

One of those features is to calculate the amount of time a user seeked a video. For this, the details of a video is also required. As such information wasn't provided, a project named `Download'(package name `fetch\_video\_information') was written to extract the details of a video.

The table created for this purpose was `video\_information'. The steps involved in this program are:
\begin{enumerate}
\item {} 
Fetch the video code from the log parsed database(`load\_video' table) in the Database class.

\item {} 
Pass this video code into the URL (in the Down class): \href{http://gdata.youtube.com/feeds/api/videos/}{http://gdata.youtube.com/feeds/api/videos/}``+video\_code+''?v=2\&alt=jsonc Example - \href{http://gdata.youtube.com/feeds/api/videos/dXb3Tx8V4hU?v=2\&alt=jsonc}{http://gdata.youtube.com/feeds/api/videos/dXb3Tx8V4hU?v=2\&alt=jsonc} opens the following :-

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZdq{}apiVersion\PYGZdq{}:\PYGZdq{}2.1\PYGZdq{},\PYGZdq{}data\PYGZdq{}:\PYGZob{}\PYGZdq{}id\PYGZdq{}:\PYGZdq{}dXb3Tx8V4hU\PYGZdq{},\PYGZdq{}uploaded\PYGZdq{}:\PYGZdq{}2013\PYGZhy{}02\PYGZhy{}23T11:
16:41.000Z\PYGZdq{},\PYGZdq{}updated\PYGZdq{}:\PYGZdq{}2013\PYGZhy{}02\PYGZhy{}23T11:16:41.000Z\PYGZdq{},\PYGZdq{}uploader\PYGZdq{}:\PYGZdq{}aakashlab\PYGZdq{},
\PYGZdq{}category\PYGZdq{}:\PYGZdq{}People\PYGZdq{},\PYGZdq{}title\PYGZdq{}:\PYGZdq{}Android UI and Layouts part 2\PYGZdq{},\PYGZdq{}description\PYGZdq{}
:\PYGZdq{}\PYGZdq{},\PYGZdq{}thumbnail\PYGZdq{}:\PYGZob{}\PYGZdq{}sqDefault\PYGZdq{}:\PYGZdq{}http://i1.ytimg.com/vi/dXb3Tx8V4hU/default.
jpg\PYGZdq{},\PYGZdq{}hqDefault\PYGZdq{}:\PYGZdq{}http://i1.ytimg.com/vi/dXb3Tx8V4hU/hqdefault.jpg\PYGZdq{}\PYGZcb{},
\PYGZdq{}player\PYGZdq{}:\PYGZob{}\PYGZdq{}default\PYGZdq{}:\PYGZdq{}http://www.youtube.com/watch?v=dXb3Tx8V4hU\PYGZam{}feature
=youtube\PYGZus{}gdata\PYGZus{}player\PYGZdq{},\PYGZdq{}mobile\PYGZdq{}:\PYGZdq{}http://m.youtube.com/details?v=dXb3Tx8
V4hU\PYGZdq{}\PYGZcb{},\PYGZdq{}content\PYGZdq{}:\PYGZob{}\PYGZdq{}5\PYGZdq{}:\PYGZdq{}http://www.youtube.com/v/dXb3Tx8V4hU?version=3\PYGZam{}f
=videos\PYGZam{}app=youtube\PYGZus{}gdata\PYGZdq{},\PYGZdq{}1\PYGZdq{}:\PYGZdq{}rtsp://r2\PYGZhy{}\PYGZhy{}\PYGZhy{}sn\PYGZhy{}a5m7zu7z.c.youtube.com/C
iILENy73wIaGQkV4hUfT\PYGZus{}d2dRMYDSANFEgGUgZ2aWRlb3MM/0/0/0/video.3gp\PYGZdq{},\PYGZdq{}6\PYGZdq{}:\PYGZdq{}r
tsp://r2\PYGZhy{}\PYGZhy{}\PYGZhy{}sn\PYGZhy{}a5m7zu7z.c.youtube.com/CiILENy73wIaGQkV4hUfT\PYGZus{}d2dRMYESARFE
gGUgZ2aWRlb3MM/0/0/0/video.3gp\PYGZdq{}\PYGZcb{},\PYGZdq{}duration\PYGZdq{}:308,\PYGZdq{}viewCount\PYGZdq{}:371,\PYGZdq{}favori
teCount\PYGZdq{}:0,\PYGZdq{}commentCount\PYGZdq{}:0,\PYGZdq{}accessControl\PYGZdq{}:\PYGZob{}\PYGZdq{}comment\PYGZdq{}:\PYGZdq{}allowed\PYGZdq{},\PYGZdq{}comme
ntVote\PYGZdq{}:\PYGZdq{}allowed\PYGZdq{},\PYGZdq{}videoRespond\PYGZdq{}:\PYGZdq{}moderated\PYGZdq{},\PYGZdq{}rate\PYGZdq{}:\PYGZdq{}allowed\PYGZdq{},\PYGZdq{}embed\PYGZdq{}:\PYGZdq{}
allowed\PYGZdq{},\PYGZdq{}list\PYGZdq{}:\PYGZdq{}allowed\PYGZdq{},\PYGZdq{}autoPlay\PYGZdq{}:\PYGZdq{}allowed\PYGZdq{},\PYGZdq{}syndicate\PYGZdq{}:\PYGZdq{}allowed\PYGZdq{}\PYGZcb{}\PYGZcb{}\PYGZcb{}
\end{Verbatim}

\item {} 
The above URL opens a page containing the JSON object about that video. So, next we downloaded this piece of information into a file.

\end{enumerate}

The code snippet for the same looks like

\begin{Verbatim}[commandchars=\\\{\}]
URL url = new URL(\PYGZdq{}http://gdata.youtube.com/feeds/api/videos/
\PYGZdq{}+video\PYGZus{}code+\PYGZdq{}?v=2\PYGZam{}alt=jsonc\PYGZdq{});

    Snippet ::

    Scanner s = new Scanner(url.openStream());
    String line;
    while(s.hasNext())
    \PYGZob{}
    line=s.nextLine();
    File file = new File(\PYGZdq{}/home/dell/workspace/
    Download/src/videoJson.json\PYGZdq{});

    // if file doesnt exists, then create it
    if (!file.exists())
    \PYGZob{}
        file.createNewFile();
    \PYGZcb{}

    FileWriter fw = new FileWriter(file.getAbsoluteFile());
    BufferedWriter bw = new BufferedWriter(fw);
    bw.write(line);
    bw.close();

    System.out.println(line);

    JsonParser.parseJson(video\PYGZus{}code);
    \PYGZcb{}
\end{Verbatim}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Then, we parsed out the objects, title and duration from this JSON object (in the JsonParser class).

\end{enumerate}

The code snippet for the same looks like

\begin{Verbatim}[commandchars=\\\{\}]
JSONObject obj = new JSONObject(jsonStr);
String title = obj.getJSONObject(\PYGZdq{}data\PYGZdq{}).getString(\PYGZdq{}title\PYGZdq{});
System.out.println(title);
JSONObject obj2 = new JSONObject(jsonStr);
int duration = (int) obj2.getJSONObject(\PYGZdq{}data\PYGZdq{}).get(\PYGZdq{}duration\PYGZdq{});
System.out.println(duration);
Database.putdata(video\PYGZus{}code,title,duration);
\end{Verbatim}
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
Finally this information was stored back in the table `video\_information' (in the Database class).

\end{enumerate}

This process was repeated for all video codes.(Run the Download only when new entries are required)

Connection with hive was made by using the Connect class.

After the video\_information table is ready, the main queries for extraction of seek time can be implemented.
\begin{quote}\begin{description}
\item[{Extracting the amount of time a student seeked(or skipped) a video}] \leavevmode
\end{description}\end{quote}

This feature is concerned with extracting the amount of time a student has skipped a portion of the video. If a student is seeking a video more than the amount of his/her viewed time, then the student is likely not interested in the course (But it is also possible that a student is skipping one video only beacuse he/she has some knowledge about that topic. This is difficult to track beacuse we cannot estimate a student's knowledge on a specific topic. And it is very rare to find a student who will seek almost all the videos in a given course, provided that he/she already knows about this topic. In that case the student wouldn't have selected the course. As we are keeping track of the seek time of all videos in a course for each student, the case of a student seeking just one or two odd videos beacause he/she had some previous knowledge in it will be handled in the mapping function later described in futher topic).

The  java class written for this purpose is Feature\_seek\_time. The following snippet fulfills the above described functionality ::

\begin{Verbatim}[commandchars=\\\{\}]
statement.executeQuery(\PYGZdq{}drop table temp0\PYGZdq{});
statement.executeQuery(\PYGZdq{}create table temp0(code string,username
string,seek int)\PYGZdq{});
statement.executeQuery(\PYGZdq{}create table seek\PYGZus{}time\PYGZus{}total(code string,
username string,seek int,title
string,duration int)\PYGZdq{});
statement.executeQuery(\PYGZdq{}insert into table temp0 select sv.code,
sv.username,sum(sv.new\PYGZus{}time\PYGZhy{}sv.old\PYGZus{}time)
from seek\PYGZus{}video sv where log\PYGZus{}id\PYGZgt{}=\PYGZdq{}+start+\PYGZdq{} and log\PYGZus{}id\PYGZlt{}\PYGZdq{}+end+\PYGZdq{} and
not exists(select * from temp0 t2
where sv.code=t2.code and sv.username=t2.username) group by sv.code,
sv.username \PYGZdq{});
resultset=statement.executeQuery(\PYGZdq{}select * from temp0\PYGZdq{});
statement.executeQuery(\PYGZdq{}insert overwrite table seek\PYGZus{}time\PYGZus{}total select
a.code,a.username,a.seek,
b.title,b.duration from temp0 a join  video\PYGZus{}information b on
a.code=b.code\PYGZdq{});
resultset=statement.executeQuery(\PYGZdq{}select * from seek\PYGZus{}time\PYGZus{}total\PYGZdq{});
while(resultset.next())
\PYGZob{}
System.out.println(resultset.getString(1)+\PYGZdq{}\PYGZbs{}t\PYGZdq{}+resultset.
getString(2)+\PYGZdq{}\PYGZbs{}t\PYGZdq{}+resultset
.getString(3)+\PYGZdq{}\PYGZbs{}t\PYGZdq{}+resultset.getString(4)+\PYGZdq{}\PYGZbs{}t\PYGZdq{}+resultset.
getString(5)+\PYGZdq{}\PYGZbs{}t\PYGZdq{});
\PYGZcb{}
statement.executeQuery(\PYGZdq{}insert overwrite table status select

name, instring, case when
name=\PYGZsq{}seek\PYGZus{}time\PYGZus{}processed\PYGZsq{} then \PYGZdq{}+end+\PYGZdq{} else inint end as
inint from status\PYGZdq{});
\end{Verbatim}

The sample output space looks like :
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|L|L|L|}
\hline
\textsf{\relax 
code
} & \textsf{\relax 
username
} & \textsf{\relax 
seek
} & \textsf{\relax 
title
} & \textsf{\relax 
duration
}\\
\hline
RU2qJTO0Gms
 & 
ak
 & 
764
 & 
IntroductionToAndroidPart1
 & 
927
\\

RU2qJTO0Gms
 & 
sachin
 & 
696
 & 
IntroductionToAndroidPart1
 & 
927
\\

KdX4DaFRAKU
 & 
ak
 & 
440
 & 
Android UI and Layouts part-1
 & 
415
\\

KdX4DaFRAKU
 & 
oshin
 & 
26
 & 
Android UI and Layouts part-1
 & 
415
\\

2E\_KTtnbzVU
 & 
sachin
 & 
269
 & 
Android UI and Layouts part 3
 & 
395
\\

aI1uMZMmnY8
 & 
sachin
 & 
181
 & 
Android UI and Layouts part 5
 & 
351
\\

d45uLZEU5U0
 & 
oshin
 & 
758
 & 
Introduction To Android Part2
 & 
782
\\
\hline\end{tabulary}

\end{quote}

To execute this query, we used an intermediate table named temp0. The table temp0 holds the 11 digit code of the video, username and seek time of a user whose entry for a particular video code is not present in the table. The seek time has been claculated by the difference in the new\_time and old\_time fields in the seek\_video table. Only those entries are considered while query execution which have not been processed yet. This has been taken care by the status table entry `seek\_time\_processed' which contains the log\_id of the user who's entry has been processed last. This is yet another example of an incremental query which uses two variables `start' and `end' to implement this concept.

Then the table `seek\_time\_total' finally contains the code, username, seek time(in seconds), title and duration of the video(in seconds). The video\_information table gives the details about the title and duration of the video in seconds.
\begin{quote}\begin{description}
\item[{Points to be noted}] \leavevmode
\end{description}\end{quote}
\begin{enumerate}
\item {} 
The statements drop table temp0, creation of temp0 and seek\_time should be executed for the first time only and then comment these lines once done.

\item {} 
Also, if a student has seen the video completely then a pause event is generated and no special event as to whether he/she has completed watching the video or not is not generated. Thus there will be a problem when a student has almost seen the video and also when the video will be watched multiple times.

\end{enumerate}

\emph{Image for downloading video information}
\begin{figure}[htbp]
\centering

\scalebox{0.600000}{\includegraphics{download.png}}
\end{figure}

\emph{Image for seek-time}
\begin{figure}[htbp]
\centering

\scalebox{0.600000}{\includegraphics{seek.png}}
\end{figure}

\begin{DUlineblock}{0em}
\item[] Measure of how much each user of each course is active per day. For this the entries of log tables have been taken.Two types of events have been considered :-
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{play\_video}] \leavevmode
\end{description}\end{quote}

\begin{DUlineblock}{0em}
\item[] This activity gives an estimate of how much active an user is in case of watching video.Though this does not gives an exact measure of how active an user is in watching videos , but gives an approximate idea about how many times user plays and pauses the videos .
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{problem\_check}] \leavevmode
\end{description}\end{quote}

This activity gives an estimate of how many problems a user has attempted.The final answer of this query has been stored in the table
activity\_per\_day.
\textbar{} This table has the following columns:-
* username
* day
* course\_id
* video\_acts
* quiz\_acts
\textbar{} For implementing runtime queries , we make use of three intermediate tables namely :-
:video\_activity:
* username
* day
* course\_id
* count
* quiz\_activity
* username
* day
* course\_id
* count

\begin{DUlineblock}{0em}
\item[] These two tables contains for a particular course,particular username,particular day how many videos were played and and how many questions attempted respectively.A third table which contains a join of these two tables on username,course\_id and day contains actual count  of videos played and questions attempted by each user on each day of every courses.This table contains entries which needs to be put on the table activity\_per\_day.If the entries already exists , those are updated and those are not present,new entries are created for them.The structure of temp table is as follows :-
\end{DUlineblock}
\begin{itemize}
\item {} 
temp

\item {} 
username

\item {} 
day

\item {} 
course\_id

\item {} 
count\_video

\item {} 
count\_quiz

\end{itemize}

\begin{DUlineblock}{0em}
\item[] Since we need to create runtime queries , we should consider only those entries in the log tables which are new.So we keep a track of the log\_id which have been processed in the status table.Let this be old\_val.Maximum of log\_id is taken to be as new\_val.Only those entries are considered whose lod id are greaater than old\_val and less than or equal to new\_val.After that the status table is updated with the new\_val as the entry for log\_id already processed.
\end{DUlineblock}

\emph{Student Activity Level}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{student_level.png}}
\end{figure}

Next feature deals with the difficulty level of each question.This difficulty level of a question is based on the information that how many students have attempted that question and in how many attempts.The final result of this query is stored in the following table:-
:diff\_level:
* problem\_id
* attempts
* no\_of\_users
* level
\textbar{} Since we need to make the query runtime, we keep a note of log\_id which has been tracked till now in the status table.We fetch that value and parse only those entries from the table problem\_check\_server whose log\_id is greater than that value.The result of the query that which questions have been attempted by how many students and in how many attempts is stored in a temporary table :-

\emph{Difficulty Level}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{difficulty_level.png}}
\end{figure}
\begin{quote}\begin{description}
\item[{record}] \leavevmode
\end{description}\end{quote}
\begin{itemize}
\item {} 
problem\_id

\item {} 
username

\item {} 
attempts

\end{itemize}

\begin{DUlineblock}{0em}
\item[] From this table we can calculate how many students attempted a particular question and in total how many attempts.We can use that vaue to update the difficulty leve of the questions whose entries already exists in table difficulty\_level. The entries which do not exists , for those dummy entries are created and then updated.following id the query written for the same.
\end{DUlineblock}

Formula

\begin{Verbatim}[commandchars=\\\{\}]
Let x be the number of attempts recored against a question and
 y be the number of users involved in that.So now the difficulty level
 of a question is updated as follows:\PYGZhy{}

new level=(old\PYGZus{}level*no\PYGZus{}of\PYGZus{}users+x)/(no\PYGZus{}of\PYGZus{}users +y)

and the no\PYGZus{}of\PYGZus{}users column is updated as
no\PYGZus{}of\PYGZus{}users=no\PYGZus{}of\PYGZus{}users+y
\end{Verbatim}


\chapter{Machine Learning}
\label{document:machine-learning}
Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages. After learning, it can then be used to classify new email messages into spam and non-spam folders.

The main aim of our project is to classify the students as gaming or not gaming. Accordingly, the ITS will intervene the learning process of the gaming students and make their learning process effective. In reference to th research paper, \textbf{“Detecting Student Misuse of Intelligent Tutoring Systems” authored by Ryan Shaun Baker, Albert T. Corbett, Kenneth R. Koedinger}, their study says that students who are averted to such `gaming the system behaviour' (behavior aimed at obtaining correct answers and advancing within the tutoring curriculum by systematically taking advantage of regularities in the software’s feedback and help) learn 2/3rds as much as similar students who do not engage in such behaviors. They came up with a machine-learned latent response model that can identify whether a student is gaming the system or not. Based on these predictions, the tutor can be re-designed for such students and make their learning process effective.

Baker and his colleagues found that a student’s frequency of gaming was strongly negatively correlated with learning. According to them, understanding why students game the system will be essential to deciding how the system should respond. Ultimately, though, whatever remediation approach is chosen, it is likely to have costs as well as benefits. For instance, preventive approaches, such as changing interface widgets to make them more difficult to game or delaying successive levels of help to prevent rapid-fire usage, may reduce gaming, but at the cost of making the tutor more frustrating and less time-efficient for other students. Since many students use help effectively and seldom or never game the system, the costs of using such an approach indiscriminately may be higher than the rewards. Whichever approach we take to remediating gaming the system, the success of that approach is likely to depend on accurately and automatically detecting which students are gaming the system and which are not.

The LRM they suggested, takes 24 features as input or data source and also the predetermined value of the student `gaming or not' of a training set of 70 students. Then it uses forward selection for model selection and then finally implements iterative gradient descent to find the best model parameters. The best-fitting model had 4 parameters, and no model considered had more than 6 parameters. They also used a cross-validation techninque, LOOCV (Leave One Out Cross Validation). Finally with the ROC (Receiver Operating Characteristic) curve, they classified the student as gaming or not gaming. On this result, they applied the interventions in the ITS.

From our project's point of view, machine leaning system is trained on student's repective 3 features, so as to make it learn distinguish between students who are gaming the system and students who are not gaming the system. After learning, it can be used to classify whether a student is gaming the system or not.

In order to implement the machine learning algorithms on the features extracted by hive queries, we have to convert them into proper form (like numerical values), suitable for implementaion. To acheive this, we have mapped the query results into the following form :

username        feature1        feature2        feature3        result

This data has been stored in the table feature. For each user there is just one entry in this table and the field `result' stores the precoded data i.e, whether the student is gaming or not.

This feature of mapping deals with the question solving ability of a person.This feature not only calculates how many questions have been solved by a user in each course, rather it also delas with the difficulty level of each question solved by a user.For this , we have utilised the information from two tables record and difficulty\_level:-
\begin{quote}\begin{description}
\item[{diff\_level}] \leavevmode
\end{description}\end{quote}
\begin{itemize}
\item {} 
problem\_id

\item {} 
attempts

\item {} 
no\_of\_users

\item {} 
level

\end{itemize}
\begin{quote}\begin{description}
\item[{record}] \leavevmode
\end{description}\end{quote}
\begin{itemize}
\item {} 
problem\_id

\item {} 
username

\item {} 
attempts

\end{itemize}

From these two tables we decideed upon a numrical value depending upon the extracted information that how many question is solved by each user of how much difficulty.This information is stored in the following intermediate table:-
\begin{quote}\begin{description}
\item[{assign}] \leavevmode
\end{description}\end{quote}
\begin{itemize}
\item {} 
username

\item {} 
value

\end{itemize}

This value has been calculated on the folloing basis

\begin{Verbatim}[commandchars=\\\{\}]
new\PYGZus{}value=old\PYGZus{}value+summation(difficulty\PYGZus{}level/attempts)/
summation(no\PYGZus{}of\PYGZus{}questions)
\end{Verbatim}

For those users, whose entries does not exists in the final table feature whose schema is expalined below,are created with dummy values.And those values are finally upadted.This feature counts in feature1 , so its value us stored in the f1 column against a particular user in the feature table:-
\begin{quote}\begin{description}
\item[{feature}] \leavevmode
\end{description}\end{quote}
\begin{itemize}
\item {} 
username

\item {} 
f1

\item {} 
f2

\item {} 
f3

\item {} 
total

\end{itemize}

\begin{DUlineblock}{0em}
\item[] Following is the query for the same ::
\end{DUlineblock}
\begin{quote}

stmt.executeQuery(``insert overwrite table assign select
b.username,sum(a.diff*b.attempts)
/count(*) from diff a join record b group by username'');
res=stmt.executeQuery(``select * from assign'');
while(res.next())
\{
//System.out.println(res.getString(1)+''t''+res.getString(2));
//+res.getString(3)+''t''+
res.getString(4)+res.getString(5)+''t'');
\}

stmt.executeQuery(``insert into table feature select a.username,
0,0,0,0 from assign a where
not exists(select * from feature where username=a.username)   '');
res=stmt.executeQuery(``select * from assign'');
float g;

while(res.next())
\{
a=res.getString(1);
g=res.getFloat(2);
//System.out.println(res.getString(1)+''t''+res.getString(2));//+
``t''+res.getString(3)+
``t''+res.getString(4)+''t''+res.getString(4)+''t'');
stmt.executeQuery(``insert overwrite  table feature select username,
case when username=
`''+a+''' then ``+g+'' else f1 end as f1,f2,f3,result from feature '');
\}
\end{quote}
\begin{quote}\begin{description}
\item[{Mapping the seek\_time feature}] \leavevmode
\end{description}\end{quote}

We have written a java class Map\_feature\_seek\_time. In this for each user, we have calculated

\begin{Verbatim}[commandchars=\\\{\}]
[sum\PYGZob{}(duration/(duration+seek))*10\PYGZcb{}]/number of videos seeked
(Say, d = duration and s = seek time.)
\end{Verbatim}

i.e, the sum of the fraction (d/(d+s)) multiplied by 10 (so that the range of a student's seek time remains within 10), divided by the total number of videos he/she has seeked.

If the grade is closer to 10 then the student is regular and seeks less else the student is seeking most of the videos.

The code snippet for the above is:

\begin{Verbatim}[commandchars=\\\{\}]
statement.executeQuery(\PYGZdq{}insert into table feature\PYGZus{}seek select
username,sum((duration/
(duration+seek))*10)/count(*) from seek\PYGZus{}time\PYGZus{}total group by username\PYGZdq{});
statement.executeQuery(\PYGZdq{}insert into table feature select a.username,
0,0,0,0 from feature\PYGZus{}seek
a where not exists(select * from feature where username=a.username)\PYGZdq{});
\end{Verbatim}

This feature is depending on the activity level of the user i. e., how much user is interacting with the system. It combines the results obtained after processing the log file and storing activity of user per day in table activity\_per\_day into a single value for per user. It is clear that students not interested in the course will have minimum activity level. also students who are trying to game the system will have high activity levels as they will constantly seek, pause videos frequently and while test they will answer the questions without contemplating over the questions.

The  java class written for this purpose is Feature\_seek\_time. The following snippet fulfills the above described functionality :-

\begin{Verbatim}[commandchars=\\\{\}]
stmt.executeQuery(\PYGZdq{}insert into table feature select apd.username,
apd.course\PYGZus{}id,0,0,0,0 from
activity\PYGZus{}per\PYGZus{}day as apd where not exists (select username,course
from feature as fe where
fe.username=apd.username and fe.course=apd.course\PYGZus{}id)\PYGZdq{});
stmt.executeQuery(\PYGZdq{}insert overwrite table tmp\PYGZus{}feature\PYGZus{}attempt select
 username,course\PYGZus{}id,
(10\PYGZhy{}abs((sum(video\PYGZus{}act)\PYGZhy{}\PYGZdq{}+avg+\PYGZdq{})/(\PYGZdq{}+avg+\PYGZdq{}*count(*))*10)) as value
from activity\PYGZus{}per\PYGZus{}day group by
username,course\PYGZus{}id\PYGZdq{});
stmt.executeQuery(\PYGZdq{}insert overwrite table feature select f.username,
f.course,f.f1,f.f2,case
when f.username=apd.username and f.course=apd.course\PYGZus{}id then apd.value
else f.f3 end as f3,result
 from tmp\PYGZus{}feature\PYGZus{}attempt as apd join feature as f on apd.username=
 f.username and apd.course\PYGZus{}id=f.course\PYGZdq{});
\end{Verbatim}

This is incremental query i.e., this will only process the log entries which were not processed earlier. To accomplish task of extracting feature three steps are required. First involves  inserting dummy entries for the entries which were added newly in the log table.Second step involves calculating level of activity and storing the values of activity level in the intermediate table. Value of activity is calculated such that users having level of activity at average level of all the users will be awarded highest score i.e., 10 and as students activity level deviate from the average value of activity level of all the users their score will decrease till the lowest possible score 10. Third step involved in which the scores which were calculated for each students will now be added into the table feature.


\chapter{DATA VISUALISATION}
\label{document:data-visualisation}\begin{quote}

Data visualization or data visualisation is a modern branch of descriptive statistics. It involves the creation and study of the visual representation of data, meaning ``information that has been abstracted in some schematic form, including attributes or variables for the units of information.''

There are different approaches on the scope of data visualization. One common focus is on information presentation. In this, there are presumably two main parts of data visualization: \textbf{statistical graphics}, and thematic cartography. We here deal with statistical graphics.
\end{quote}
\begin{itemize}
\item {} 
We have written hive queries to extract statistical data from the hive database.

\item {} 
The output of the hive queries is exported to csv file (tsv file or json or xml files can also be used).

\item {} 
These files are used as input for plotting the graphs.

\item {} 
We have tried the highcharts.js javascript library to plot a varied range of charts.

\item {} 
Highcharts is a JavaScript library used for manipulating documents based on data.

\item {} 
Highcharts helps you to bring data to life using HTML, CSS, JQuery and Javascript.

\end{itemize}

Following are the Hive queries used for data extraction from Hive database.
\begin{itemize}
\item {} 
Students who have registered for a course in their first login but have never activated their account.
This includes the students who have registered on the site, browsed through the courses and then never activated their account through their mail.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select (select count(*) from auth\PYGZus{}user where is\PYGZus{}active=0)
/(select count(id)
     from auth\PYGZus{}user)*100 as pct\PYGZus{}inactive;
\end{Verbatim}

Tables used: \textbf{auth\_user}
\end{quote}

\item {} \begin{description}
\item[{Distribution of students according to their age groups}] \leavevmode
Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{}select aup.year\PYGZus{}of\PYGZus{}birth from auth\PYGZus{}user au join auth\PYGZus{}userprofile
aup on au.id=aup.user\PYGZus{}id where aup.year\PYGZus{}of\PYGZus{}birth!=0;
\end{Verbatim}

Tables used: \textbf{auth\_user,auth\_userprofile}

\end{description}

\item {} 
Dropouts (who never came back to continue the course)
Number of drop outs in the corresponding weeks.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select count(*) from auth\PYGZus{}user au where datediff
(au.last\PYGZus{}login,au.date\PYGZus{}joined)\PYGZgt{}=0
     and datediff(au.last\PYGZus{}login,au.date\PYGZus{}joined)\PYGZlt{}7;
\end{Verbatim}

Tables used: \textbf{auth\_user}

Output :
\begin{quote}

The number of students who have droped out of the course in the corresponding weeks.
\begin{quote}

Description:
\begin{quote}

The user is asked about the number of weeks for which the data is required.
The above query represents the data extracted for the first week.
The last\_login and the date\_joined corresponding to each of the user are compared and their difference is taken into consideration. This gives us the number of weeks for which the user has been inactive. A count of such users is taken for that particular week.
Similarly, the data for the further weeks is queried.
\end{quote}
\end{quote}
\end{quote}

CSV Format:
\begin{quote}

The above code implements the defined query and stores the results in a csv file: \emph{category1.csv}.
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
week1,week2,week3,week4
no. OfDropouts in week 1,no OfDropouts in week 2,no
OfDropouts in week 3,
no OfDropouts in week 4
\end{Verbatim}
\end{quote}
\begin{description}
\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}
\end{quote}
\begin{description}
\item[{Distrubution of drop outs according to their age group.}] \leavevmode
Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select count(*) from auth\PYGZus{}user au,auth\PYGZus{}userprofile aup where
     datediff(au.last\PYGZus{}login,au.date\PYGZus{}joined)\PYGZlt{}7 and au.id=aup.user\PYGZus{}id and
     aup.year\PYGZus{}of\PYGZus{}birth\PYGZgt{}=1975 and aup.year\PYGZus{}of\PYGZus{}birth\PYGZlt{}1980;
\end{Verbatim}

Tables used: \textbf{auth\_user, auth\_userprofile}

Output :
\begin{quote}

The year\_of\_birth of the students who have dropped out in the corresponding week.
\begin{quote}

Description:
\begin{quote}

The user provides with the yearGap for the range of the years for which the students are to classified and also the number of weeks for which the result is desired.
The difference between the last\_login and the date\_joined gives the number of days for which a user has been inactive.
The year\_of\_birth of the students are then taken as output to classify them into the various age groups accordingly.
\end{quote}
\end{quote}
\end{quote}

CSV Format:
\begin{quote}

The java code implements the defined query and stores the results in a csv file: \emph{category2.csv}.
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
ageGroup1,ageGroup2,ageGroup3,ageGroup4
week1,no. OfDropouts in week 1 separated by “,”
week2,no. OfDropouts in week 2 separated by “,”
week3,no. OfDropouts in week 3 separated by “,”
week4n,o. OfDropouts in week 4 separated by “,”
\end{Verbatim}
\end{quote}
\begin{description}
\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}

\end{description}

Distrubution of drop outs according to their education.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select count(*), level\PYGZus{}of\PYGZus{}education from (select * from
auth\PYGZus{}userprofile aup where aup.user\PYGZus{}id in (select au.id from
auth\PYGZus{}user au where datediff(au.last\PYGZus{}login,au.date\PYGZus{}joined)\PYGZlt{}7))y
group by y.level\PYGZus{}of\PYGZus{}education
\end{Verbatim}

Tables used: \textbf{auth\_user, auth\_userprofile}

Output :
\begin{quote}

The number of drop-outs corresponding to each level\_of\_education and each week
\begin{quote}

Description:
\begin{quote}

The user provides the number of weeks for which the data is required.
First, the total number of drop-outs corresponding to a particular week are taken.
Their id from auth\_user and auth\_userprofile are compared and then they are grouped by education so that all categories of education are obtained.
A count for each group of education is made and output.
\end{quote}
\end{quote}
\end{quote}

CSV Format:
\begin{quote}

The above code implements the defined query and stores the results in a csv file: \emph{category3.csv} .
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
levelOfEducation1,levelOfEducation2,levelOfEducation3,
levelOfEducation4
week1,no. OfDropouts in week 1 separated by “,”
week2,no. OfDropouts in week 2 separated by “,”
week3,no. OfDropouts in week 3 separated by “,”
week4n,o. OfDropouts in week 4 separated by “,”
\end{Verbatim}
\end{quote}
\begin{description}
\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}
\end{quote}

Distrubution of drop outs according to their education.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select count(*),aup.gender from auth\PYGZus{}user au join
auth\PYGZus{}userprofile aup on where au.id=aup.user\PYGZus{}id datediff
(au.last\PYGZus{}login,au.date\PYGZus{}joined)\PYGZlt{}7 group by aup.gender;
\end{Verbatim}

Tables used: \textbf{auth\_user, auth\_userprofile}

Output :
\begin{quote}

The total number of males/females drop-outs corresponding to each week.
\begin{quote}

Description:
\begin{quote}

The user provides the number of weeks for which the data is required.
The two tables are joined using the user\_id of each user.
The count is made for each category of gender and is output along with the category.
\end{quote}
\end{quote}
\end{quote}

CSV Format:
\begin{quote}

The above code implements the defined query and stores the results in a csv file: \emph{category4.csv}.
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
Males,Females
week1,no. OfDropouts in week 1 separated by “,”
week2,no. OfDropouts in week 2 separated by “,”
week3,no. OfDropouts in week 3 separated by “,”
week4n,o. OfDropouts in week 4 separated by “,”
\end{Verbatim}
\end{quote}
\begin{description}
\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}
\end{quote}

Distrubution of drop outs according to course category.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select count(*) from auth\PYGZus{}user au,auth\PYGZus{}userprofile
aup,courseware\PYGZus{}studentmodule csm where datediff(au.last\PYGZus{}login,
au.date\PYGZus{}joined)\PYGZlt{}7 and au.id=aup.user\PYGZus{}id and csm.student\PYGZus{}id=au.id
and csm.course\PYGZus{}id=\PYGZsq{}Summer\PYGZus{}Intern\PYGZus{}IIT\PYGZus{}Mumbai/SI001/2014\PYGZus{}SI\PYGZus{}May\PYGZsq{};
\end{Verbatim}

Tables used: \textbf{auth\_user, auth\_userprofile, courseware\_studentmodule}

Output :
\begin{quote}

The query returns the number of drop-out students belonging to that particular course in a particular week. Data for a series of weeks is returned through the java program.
\begin{quote}

Description:
\begin{quote}

Course\_category and the number of weeks for which the data is required are input to the java program.
The three tables are joined using the unique user\_ids.
A count of such users is made who have left in a particular week in that particular subject.
\end{quote}
\end{quote}
\end{quote}

CSV Format:
\begin{quote}

The above code implements the defined query and stores the results in a csv file: \emph{category5.csv}.
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
SI001, SI002, SI003, SI004
week1,no. OfDropouts in week 1 separated by “,”
week2,no. OfDropouts in week 2 separated by “,”
week3,no. OfDropouts in week 3 separated by “,”
week4n,o. OfDropouts in week 4 separated by “,”
\end{Verbatim}
\end{quote}
\begin{description}
\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}
\end{quote}

\item {} 
Grades of students depending on the various factors (for a particular subject):

Distrubution of grades according to location.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select sum(grade)/count(*),level\PYGZus{}of\PYGZus{}education from
courseware\PYGZus{}studentmodule join auth\PYGZus{}userprofile on student\PYGZus{}id
=user\PYGZus{}id and course\PYGZus{}id=\PYGZsq{}Summer\PYGZus{}Intern\PYGZus{}IIT\PYGZus{}Mumbai/SI001/2014\PYGZus{}SI\PYGZus{}May\PYGZsq{}
group by auth\PYGZus{}userprofile.location;
\end{Verbatim}

Tables used: \textbf{courseware\_studentmodule, auth\_userprofile}

Output :
\begin{quote}

The query returns the avarege grade corresponding to each location for every location that has been registered.
\begin{quote}

Description:
\begin{quote}

The user provides with the course\_id for which the result is required.
The two tables are joined on the unique user\_id for each user(student) and the given subject is compared in the courseware\_student module table.
The query is grouped by location and then the average grade corresponding to that particular location is obtained.
The ``sum(grade)/count(*)'' gives the average grade for each particular location.
\end{quote}
\end{quote}
\end{quote}
\begin{description}
\item[{CSV Format:}] \leavevmode
The above code implements the defined query and stores the results in a csv file: \emph{category7.csv}.
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
Line 1 : Different locations seperated by “,”
Line 2 : Average grades of corresponding locations
separated by “,”
\end{Verbatim}

\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}
\end{quote}

Distrubution of grades according to their education level.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select sum(grade)/count(*),level\PYGZus{}of\PYGZus{}education from
courseware\PYGZus{}studentmodule join auth\PYGZus{}userprofile on student\PYGZus{}id
=user\PYGZus{}id and course\PYGZus{}id=\PYGZsq{}Summer\PYGZus{}Intern\PYGZus{}IIT\PYGZus{}Mumbai/SI001/2014\PYGZus{}SI\PYGZus{}May\PYGZsq{}
group by level\PYGZus{}of\PYGZus{}education;
\end{Verbatim}

Tables used: \textbf{courseware\_studentmodule, auth\_userprofile}

Output :
\begin{quote}

The query returns the avarege grade corresponding to each education level for every education level that has been registered.
\begin{quote}

Description:
\begin{quote}

The user provides with the course\_id for which the result is required.
The two tables are joined on the unique user\_id for each user(student) and the given subject is compared in the courseware\_student module table.
The query is grouped by education level and then the average grade corresponding to that particular education level is obtained.
The ``sum(grade)/count(*)'' gives the average grade for each particular education level.
\end{quote}
\end{quote}
\end{quote}

CSV Format:
\begin{quote}

The above code implements the defined query and stores the results in a csv file: \emph{category8.csv}.
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
Line 1 : Different levels of education separated by “,”
Line 2 : Average grades of corresponding education level
separated by “,”
\end{Verbatim}
\end{quote}
\begin{description}
\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}
\end{quote}

Distrubution of grades according to gender.
\begin{quote}

Query

\begin{Verbatim}[commandchars=\\\{\}]
SQL\PYGZgt{} select sum(grade)/count(*),gender from courseware\PYGZus{}studentmodule
join auth\PYGZus{}userprofile on student\PYGZus{}id=user\PYGZus{}id and gender=\PYGZsq{}m\PYGZsq{} and
course\PYGZus{}id =\PYGZsq{}Summer\PYGZus{}Intern\PYGZus{}IIT\PYGZus{}Mumbai/SI001/2014\PYGZus{}SI\PYGZus{}May\PYGZsq{} group by gender;
\end{Verbatim}

Tables used: \textbf{courseware\_studentmodule, auth\_userprofile}

Output :
\begin{quote}

The query returns the avarege grade corresponding to each gender for every gender that has been registered.
\begin{quote}

Description:
\begin{quote}

The user provides with the course\_id for which the result is required.
The two tables are joined on the unique user\_id for each user(student) and the given subject is compared in the courseware\_student module table.
The query is grouped by gender and then the average grade corresponding to that particular gender is obtained.
The ``sum(grade)/count(*)'' gives the average grade for each particular gender.
\end{quote}
\end{quote}
\end{quote}

CSV Format:
\begin{quote}

The above code implements the defined query and stores the results in a csv file: \emph{category9.csv}.
The results are stored in the following format

\begin{Verbatim}[commandchars=\\\{\}]
Line 1 : Different genders separated by “,”
Line 2 : Average grades of corresponding gender separated by “,”
\end{Verbatim}
\end{quote}
\begin{description}
\item[{\emph{NOTE:}}] \leavevmode
The csv file serves as a input to the data visualisation.

\end{description}
\end{quote}

\end{itemize}

We have written Java code for merging the output from the above hive queries in formatted csv (comma separated values)
file. Following is a snippet for the same.

Code snipet of java program calculating distribution of drop-outs according to age group and storing it in a csv file

\begin{Verbatim}[commandchars=\\\{\}]
import java.sql.*;
import java.util.ArrayList;
import java.util.Scanner;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;

public class category2
\PYGZob{}
    private Connection connect = null;
    private Statement statement = null;
    private PreparedStatement preparedStatement = null;
    private ResultSet resultSet = null;
    Scanner in = new Scanner(System.in);
    private int yearGap;
    private int numberOfWeeks;
    ArrayList\PYGZlt{}Integer\PYGZgt{} yearOfBirth = new ArrayList\PYGZlt{}Integer\PYGZgt{}();
    private int arraySize;
    private int minBirthYear;
    private int maxBirthYear;
    private int[][] ageGroup;
    private int flag;
    private int i;
    private int j;
    private int k;
    private double tmp;
    private int start;
    private int end;

    public static void main(String args[])
    \PYGZob{}
        category2 fa = new category2();
        fa.attempts();
    \PYGZcb{}
    public void attempts()
    \PYGZob{}
        try
        \PYGZob{}
            connect = Connect.GetConnection();
            statement = connect.createStatement();

            statement.executeQuery(\PYGZdq{}use edxapp1\PYGZdq{});
            /*resultSet = statement.executeQuery(\PYGZdq{}select distinct
            student\PYGZus{}id from courseware\PYGZus{}studentmodule\PYGZdq{});
            System.out.println(\PYGZdq{}after query execution\PYGZdq{});
            while(resultSet.next())
            \PYGZob{}
                System.out.println(resultSet.getString(1));
                System.out.println(\PYGZdq{}Inside while\PYGZdq{});
            \PYGZcb{}*/



            /*
             *
             *
             * Drop\PYGZhy{}outs according to age group(birth year)
             *
             * */
            /*
             * get the number of weeks
             * set the start and end of each week:start,end
             * run query for each week
             * store result of each week accordingly
             *
             * */
            System.out.println(\PYGZdq{}Enter the age group range:\PYGZdq{});
            yearGap = in.nextInt();
            System.out.println(\PYGZdq{}Enter the number of weeks for which
            the data is required:\PYGZdq{});
            numberOfWeeks = in.nextInt();
            i=numberOfWeeks;
            resultSet = statement.executeQuery(\PYGZdq{}select aup.year\PYGZus{}of\PYGZus{}birth
            from auth\PYGZus{}user au
            join auth\PYGZus{}userprofile aup on au.id=aup.user\PYGZus{}id where
            aup.year\PYGZus{}of\PYGZus{}birth!=0\PYGZdq{});
            resultSet.next();
            minBirthYear=resultSet.getInt(1);
            maxBirthYear=resultSet.getInt(1);
            do\PYGZob{}
                if(minBirthYear\PYGZgt{}resultSet.getInt(1))
                \PYGZob{}
                    minBirthYear=resultSet.getInt(1);
                \PYGZcb{}
                if(maxBirthYear\PYGZlt{}resultSet.getInt(1))
                \PYGZob{}
                    maxBirthYear=resultSet.getInt(1);
                \PYGZcb{}
            \PYGZcb{}while(resultSet.next());

            System.out.println(minBirthYear);
            System.out.println(maxBirthYear);
            tmp = (maxBirthYear\PYGZhy{}minBirthYear)/yearGap;
            System.out.println(\PYGZdq{}tmp :\PYGZdq{}+tmp);
            tmp = Math.ceil(tmp);
            System.out.println(\PYGZdq{}tmp :\PYGZdq{}+tmp);
            tmp++;
            ageGroup = new int[numberOfWeeks+1][(int) tmp];
            //System.out.println(\PYGZdq{}age group length is:\PYGZdq{}+ageGroup.length);


            start=0;
            end=7;
            i=0;
            while(i\PYGZlt{}numberOfWeeks)
            \PYGZob{}
                System.out.println(\PYGZdq{}inside number of weeks\PYGZdq{});
                resultSet = statement.executeQuery(\PYGZdq{}select aup.year\PYGZus{}of\PYGZus{}birth
                from auth\PYGZus{}user au join auth\PYGZus{}userprofile aup on au.id=
                aup.user\PYGZus{}id where datediff (au.last\PYGZus{}login,au.date\PYGZus{}joined)\PYGZgt{}=
                \PYGZdq{}+start+\PYGZdq{} and datediff(au.last\PYGZus{}login,au.date\PYGZus{}joined)
                \PYGZlt{}\PYGZdq{}+end+\PYGZdq{} and aup.year\PYGZus{}of\PYGZus{}birth!=0\PYGZdq{});

                while(resultSet.next())
                \PYGZob{}
                    //System.out.println(\PYGZdq{}**********\PYGZdq{});
                    yearOfBirth.add(resultSet.getInt(1));
                    System.out.println(resultSet.getInt(1));
                \PYGZcb{}



                flag=minBirthYear;
                arraySize = yearOfBirth.size();
                System.out.println(arraySize);

                j=0;
                //iterate over each age group
                while(flag\PYGZlt{}maxBirthYear)
                \PYGZob{}
                    k=0;
                    //iterate over all the year\PYGZus{}of\PYGZus{}births retrieved
                    while(k\PYGZlt{}arraySize)
                    \PYGZob{}
                        if((int)yearOfBirth.get(k)\PYGZgt{}=flag \PYGZam{}\PYGZam{}
                        (int)yearOfBirth.get(k)\PYGZlt{}flag+yearGap)
                        \PYGZob{}
                            ageGroup[i][j]++;
                        \PYGZcb{}
                        k++;
                    \PYGZcb{}
                    System.out.print(ageGroup[i][j]+\PYGZdq{}\PYGZbs{}t\PYGZdq{});
                    j++;
                    flag+=yearGap;
                \PYGZcb{}

                yearOfBirth.clear();
                System.out.println();

                i++;
                start+=7;
                end+=7;
            \PYGZcb{}
            //System.out.println(\PYGZdq{}here 1\PYGZdq{});


            //put the years in the appropriate categories

            //write to file


        \PYGZcb{}
        catch(Exception ex)
        \PYGZob{}
            System.out.println(ex);
        \PYGZcb{}
        finally
        \PYGZob{}
            try
            \PYGZob{}
                connect.close();
            \PYGZcb{}
            catch (SQLException e)
            \PYGZob{}
                e.printStackTrace();
            \PYGZcb{}
        \PYGZcb{}
        try \PYGZob{}
            String content = \PYGZdq{}\PYGZdq{};
            String content1 = \PYGZdq{}\PYGZdq{};
            //System.out.println(\PYGZdq{}\PYGZbs{}nageGroup length:\PYGZbs{}t\PYGZdq{}+ageGroup.length+\PYGZdq{}\PYGZbs{}n\PYGZdq{});
            //j=ageGroup.length;
            i=0;
            /*while(i\PYGZlt{}numberOfWeeks)
            \PYGZob{}
                content=content.concat(\PYGZdq{}age group\PYGZdq{}+(i+1));
                if(i!=numberOfWeeks\PYGZhy{}1)
                \PYGZob{}
                    content=content.concat(\PYGZdq{},\PYGZdq{});
                \PYGZcb{}
                else
                \PYGZob{}
                    content=content.concat(\PYGZdq{}\PYGZbs{}n\PYGZdq{});
                \PYGZcb{}
                content1=content1+ageGroup[i];
                if(i!=numberOfWeeks\PYGZhy{}1)
                \PYGZob{}
                    content1=content1.concat(\PYGZdq{},\PYGZdq{});
                \PYGZcb{}
                i++;
            \PYGZcb{}*/
            i=0;
            while(i\PYGZlt{}tmp)
            \PYGZob{}
                content=content.concat(\PYGZdq{}age group\PYGZdq{}+(i+1));
                if(i!=tmp\PYGZhy{}1)
                \PYGZob{}
                    content=content.concat(\PYGZdq{},\PYGZdq{});
                \PYGZcb{}
                else
                \PYGZob{}
                    content=content.concat(\PYGZdq{}\PYGZbs{}n\PYGZdq{});
                \PYGZcb{}
                i++;
            \PYGZcb{}
            i=0;
            while(i\PYGZlt{}numberOfWeeks)
            \PYGZob{}
                j=0;
                content1=content1.concat(\PYGZdq{}week\PYGZdq{}+(i+1));
                content1=content1.concat(\PYGZdq{},\PYGZdq{});
                while(j\PYGZlt{}tmp)
                \PYGZob{}
                    content1=content1+ageGroup[i][j];
                    if(j!=tmp\PYGZhy{}1)
                    \PYGZob{}
                        content1=content1.concat(\PYGZdq{},\PYGZdq{});
                    \PYGZcb{}
                    else
                    \PYGZob{}
                        content1=content1.concat(\PYGZdq{}\PYGZbs{}n\PYGZdq{});
                    \PYGZcb{}
                    j++;
                \PYGZcb{}
                i++;
            \PYGZcb{}

            File file = new File(\PYGZdq{}/home/rounak/hadoopData/cvs files/cat2.csv\PYGZdq{});

            // if file doesn\PYGZsq{}t exists, then create it
            if (!file.exists()) \PYGZob{}
                file.createNewFile();
            \PYGZcb{}

            FileWriter fw = new FileWriter(file.getAbsoluteFile());
            BufferedWriter bw = new BufferedWriter(fw);
            bw.write(content);
            bw.write(content1);
            System.out.println(\PYGZdq{}file written\PYGZdq{});
            bw.close();
        \PYGZcb{}catch(IOException e)\PYGZob{}
            e.printStackTrace();
        \PYGZcb{}



    \PYGZcb{}
\PYGZcb{}
\end{Verbatim}

Generating interactive charts using Highcharts library.
\begin{itemize}
\item {} 
We have used Highcharts, a Javascript library for generating statististical charts.

\item {} 
To generate the charts we require the \emph{highcharts.js} file in our project. Also, we will be needing the \emph{jquery} library.
Thus, Include the following anywhere in head/body of your base template

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}script src=\PYGZdq{}\PYGZob{}\PYGZpc{} static \PYGZsq{}js/highcharts.js\PYGZsq{} \PYGZpc{}\PYGZcb{}\PYGZdq{}\PYGZgt{}\PYGZlt{}/script\PYGZgt{}
\PYGZlt{}script src=\PYGZdq{}\PYGZob{}\PYGZpc{} static \PYGZsq{}js/modules/exporting.js\PYGZsq{} \PYGZpc{}\PYGZcb{}\PYGZdq{}\PYGZgt{}
\PYGZlt{}/script\PYGZgt{} \PYGZlt{}!\PYGZhy{}\PYGZhy{} Comes with highcharts \PYGZhy{}\PYGZhy{}\PYGZgt{}
\PYGZlt{}script type=\PYGZdq{}text/javascript\PYGZdq{}
src=\PYGZdq{}\PYGZob{}\PYGZpc{} static \PYGZsq{}js/jquery.min.js\PYGZsq{} \PYGZpc{}\PYGZcb{}\PYGZdq{}\PYGZgt{}\PYGZlt{}/script\PYGZgt{}
\end{Verbatim}

\item {} 
To fetch data from a csv file we need to write our own script, wherein using jquery we fetch the data from csv file and manipulate it to stor the data
accordingly in the data modules of Highchart data-object.
Following is a code-snippet in Jquery

\emph{Snippet}

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{}.get(\PYGZdq{}\PYGZob{}\PYGZpc{} static \PYGZsq{}data/cat4.csv\PYGZsq{} \PYGZpc{}\PYGZcb{}\PYGZdq{}, function(data) \PYGZob{}
    // Split the lines
    var lines = data.split(\PYGZsq{}\PYGZbs{}n\PYGZsq{});
    \PYGZdl{}.each(lines, function(lineNo, line) \PYGZob{}
        var items = line.split(\PYGZsq{},\PYGZsq{});

        // header line containes categories
        if (lineNo == 0) \PYGZob{}
            \PYGZdl{}.each(items, function(itemNo, item) \PYGZob{}
                if (itemNo \PYGZgt{} 0) options.xAxis.categories.push(item);
            \PYGZcb{});
        \PYGZcb{}

        // the rest of the lines contain data with their
        //name in the first position
        else \PYGZob{}
            var series = \PYGZob{}
                data: []
            \PYGZcb{};
            \PYGZdl{}.each(items, function(itemNo, item) \PYGZob{}
                if (itemNo == 0) \PYGZob{}
                    series.name = item;
                \PYGZcb{} else \PYGZob{}
                    series.data.push(parseFloat(item));
                \PYGZcb{}
            \PYGZcb{});

            options.series.push(series);

        \PYGZcb{}

    \PYGZcb{});
    var chart = new Highcharts.Chart(options);
\PYGZcb{});
\end{Verbatim}

Below given is the snippet indicating the creation of a Highchart chart object with data being rendered from
a csv file using Jquery (\emph{shown above})

\emph{Snippet}

\begin{Verbatim}[commandchars=\\\{\}]
var options = \PYGZob{}
    chart: \PYGZob{}
        renderTo: \PYGZsq{}container\PYGZsq{},
        type: \PYGZsq{}column\PYGZsq{}
    \PYGZcb{},
    title: \PYGZob{}
        text: \PYGZsq{}Dropout Rate vs Gender (week\PYGZhy{}wise)\PYGZsq{}
    \PYGZcb{},
    xAxis: \PYGZob{}
        title: \PYGZob{}
            enabled: true,
            text: \PYGZsq{}Gender\PYGZsq{},
            x: \PYGZhy{}20
        \PYGZcb{},
        categories: []
    \PYGZcb{},
    yAxis: \PYGZob{}
        title: \PYGZob{}
            text: \PYGZsq{}Dropout Rate\PYGZsq{}
        \PYGZcb{}
    \PYGZcb{},
    series: []
\PYGZcb{};
\end{Verbatim}

Both of the snippets go in the code

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{}(document).ready(function() \PYGZob{}

    /* Snippet goes here */

\PYGZcb{});
\end{Verbatim}

\end{itemize}

Below are the charts that we have generated.
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{ageGroup.png}}
\end{figure}

\emph{This pie-chart represents the distribution of students according to their age group}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{cat1.png}}
\end{figure}

\emph{This chart represents the total number of drop-outs corresponding to each week}
\emph{X-axis : Weeks}
\emph{Y-axis : Drop-outs per week}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{cat2.png}}
\end{figure}

\emph{This chart represents the total number of drop-outs corresponding to each age group}
\emph{X-axis : Weeks for each age group}
\emph{Y-axis : Drop-out rate}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{cat3.png}}
\end{figure}

\emph{This chart represents the total number of drop-outs corresponding to each week}
\emph{X-axis : Weeks for each age group}
\emph{Y-axis : Drop-out rate}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{cat4.png}}
\end{figure}

\emph{This chart represents the total number of drop-outs corresponding to each gender}
\emph{X-axis : Weeks for each gender}
\emph{Y-axis : Drop-out rate}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{cat8.png}}
\end{figure}

\emph{This chart represents the average grade corresponding to each level of education}
\begin{figure}[htbp]
\centering

\scalebox{0.700000}{\includegraphics{cat9.png}}
\end{figure}

\emph{This chart represents the average grade corresponding to level of education}
\emph{X-axis : Possible number of Gender}
\emph{Y-axis : Average grades for each gender}


\chapter{Future work}
\label{references:future-work}\label{references::doc}
Many new features can be added in future in this project.The edx software which we used in our project , didn't have any hint option.The frequency with which a student uses hint can be of great help in determining whether a student is gaming or not.For example , if a student is using hints in all questions , that means he is gaming and is doing the course on;y get a certificate.Another case can be , how quickly is he taking hint on a particular question.It is apparent that if he is quickly clicking on hint button to get the next hint , he is not utilizing previous hints to answer the question.Secondly if we could utilize the actual database of edx  then we would have many other information such as how total number of module in  a course , which video is related to which set of questions.These things could help us determine which student watched video first and which student attempted questions first.This could be of great use.Thirdly , we have not taken into account the fact that a student could have watched same video many times.We are simply taking sum of the time , a student has actually watched videos.We can also include factors such as hoe many times a video has been loaded by that student and how what is the maximum length of video which he has seen.This should be updated only when that student watches that video which exceeds that time.
Next thing is that we have used WEKA software to implement machine learning.Our next aim is to write programs of machine learning on our own so that it will apply to our case in the best way.And the last thing is the intervention.The system which which we have designed is runtime.That is the log entries are parsed as they are coming into the log file.So we can determine at that particular time that these students are gaming the system and we can take some actions against only those students.We are looking forward in future to implement this system and turn this system to a fully working system , which can practically be applied to ITS in order to control the gaming of the systems.


\chapter{Conclusion}
\label{references:conclusion}
We have designed a system , which given the log entries of edx , can determine whether a student  is gaming or not.This system , if implemented by ITS will help them in creating trustworthy courses.ITS can ensure that certificates are assigned to only those students who have completed their course honestly without gaming.The system can be of great use in future and will surely find its place in ITS.This project can be improved greatly , if we can utilize many other information of edx database.We are looking in future to implement many more features as well as include the interventions in the proper way doc.


\chapter{References}
\label{references:references}
\begin{DUlineblock}{0em}
\item[] 1. Off-Task Behavior in the Cognitive Tutor Classroom: When Students “Game the System”
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] Ryan Shaun Baker, Albert T. Corbett,Kenneth R. Koedinger, Angela Z. Wagner Human-Computer Interaction Institute, Carnegie Mellon University Pittsburgh, PA, USA
\item[] \href{http://www.columbia.edu/~rsb2162/p383-baker-rev.pdf}{http://www.columbia.edu/\textasciitilde{}rsb2162/p383-baker-rev.pdf}
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 2. Detecting Student Misuse of Intelligent Tutoring Systems
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] Ryan Shaun Baker, Albert T. Corbett, Kenneth R. Koedinger Human-Computer Interaction Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, 15217, USA
\item[] \href{http://www.columbia.edu/~rsb2162/BCK2004MLFinal.pdf}{http://www.columbia.edu/\textasciitilde{}rsb2162/BCK2004MLFinal.pdf}
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 3. Apache Hive documentation
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] \href{https://cwiki.apache.org/confluence/display/Hive/Home\#Home-UserDocumentation}{https://cwiki.apache.org/confluence/display/Hive/Home\#Home-UserDocumentation}
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 4. Apache Hadoop documentation
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] \href{http://hadoop.apache.org/docs/r0.18.2/}{http://hadoop.apache.org/docs/r0.18.2/}
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 5. Apache Sqoop documentation
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] \href{http://sqoop.apache.org/docs/1.4.1-incubating/}{http://sqoop.apache.org/docs/1.4.1-incubating/}
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 6. Setting up single node and multi node cluster on ubuntu platform using hadoop
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] \href{http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/}{http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/}
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 7. Habits of Effective Sqoop Users
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] Kate Ting, Customer Operations Engineer, \href{mailto:kate@cloudera.com}{kate@cloudera.com}
\item[] sqoop\_meetup\_kate\_ting\_110711.pdf
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 8. Edureka video tutorials
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] 2013-12-09 08.04 Big Data and Hadoop class 1.mp4
\item[] 2013-12-10 08.03 Big Data and Hadoop class 2.mp4
\item[] Module-1 Assignment - Number of Data Node Calculation.pdf
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 9. Edx database schema and log events format
\item[]
\begin{DUlineblock}{\DUlineblockindent}
\item[] Student Information and Progress Database:
\item[] \href{http://edx.readthedocs.org/projects/devdata/en/latest/internal\_data\_formats/sql\_schema.html}{http://edx.readthedocs.org/projects/devdata/en/latest/internal\_data\_formats/sql\_schema.html}
\item[] Tracking log:
\item[] \href{http://edx.readthedocs.org/projects/devdata/en/latest/internal\_data\_formats/tracking\_logs.html\#video}{http://edx.readthedocs.org/projects/devdata/en/latest/internal\_data\_formats/tracking\_logs.html\#video}
\end{DUlineblock}
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 10. Book: Hadoop - The Definitive Guide
\end{DUlineblock}


\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\emph{genindex}

\item {} 
\emph{modindex}

\item {} 
\emph{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}
